{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from CifarDataset import CifarDataset\n",
    "from IntelDataset import IntelDataset\n",
    "from TrainModel import TrainModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "class num: 10\n",
      "ilość klas: 10\n",
      "wielkość po warstawach conv: 5\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv2d-1     [1, 16, 32, 32]           1,216           1,216\n",
      "       MaxPool2d-2     [1, 16, 17, 17]               0               0\n",
      "          Conv2d-3     [1, 32, 17, 17]          12,832          12,832\n",
      "          Conv2d-4     [1, 64, 17, 17]          51,264          51,264\n",
      "          Conv2d-5     [1, 86, 17, 17]         137,686         137,686\n",
      "       MaxPool2d-6       [1, 86, 9, 9]               0               0\n",
      "          Conv2d-7      [1, 128, 9, 9]         275,328         275,328\n",
      "          Conv2d-8      [1, 128, 9, 9]         409,728         409,728\n",
      "          Conv2d-9      [1, 128, 9, 9]         409,728         409,728\n",
      "      MaxPool2d-10      [1, 128, 5, 5]               0               0\n",
      "         Conv2d-11      [1, 128, 5, 5]         409,728         409,728\n",
      "         Linear-12             [1, 64]         204,864         204,864\n",
      "         Linear-13             [1, 16]           1,040           1,040\n",
      "         Linear-14             [1, 10]             170             170\n",
      "=======================================================================\n",
      "Total params: 1,913,584\n",
      "Trainable params: 1,913,584\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "GPU is available\n",
      "pk: 31.25 %\n",
      "Learning rate: 0.0010000000\n",
      "Epoch: 0............. Loss: 1.7939\n",
      "pk: 46.14 %\n",
      "Learning rate: 0.0010400000\n",
      "Epoch: 1............. Loss: 1.3159\n",
      "pk: 55.24 %\n",
      "Learning rate: 0.0010816000\n",
      "Epoch: 2............. Loss: 1.1740\n",
      "pk: 61.14 %\n",
      "Learning rate: 0.0011248640\n",
      "Epoch: 3............. Loss: 1.0057\n",
      "pk: 62.65 %\n",
      "Learning rate: 0.0011698586\n",
      "Epoch: 4............. Loss: 1.0143\n",
      "pk: 67.74 %\n",
      "Learning rate: 0.0012166529\n",
      "Epoch: 5............. Loss: 0.8479\n",
      "pk: 69.73 %\n",
      "Learning rate: 0.0012653190\n",
      "Epoch: 6............. Loss: 0.7958\n",
      "pk: 69.97 %\n",
      "Learning rate: 0.0013159318\n",
      "Epoch: 7............. Loss: 0.9726\n",
      "pk: 71.43 %\n",
      "Learning rate: 0.0013685691\n",
      "Epoch: 8............. Loss: 0.8131\n",
      "pk: 72.65 %\n",
      "Learning rate: 0.0014233118\n",
      "Epoch: 9............. Loss: 0.8669\n",
      "pk: 74.02 %\n",
      "Learning rate: 0.0014802443\n",
      "Epoch: 10............. Loss: 0.7906\n",
      "pk: 72.45 %\n",
      "Learning rate: 0.0015394541\n",
      "Epoch: 11............. Loss: 0.8359\n",
      "pk: 73.86 %\n",
      "Learning rate: 0.0016010322\n",
      "Epoch: 12............. Loss: 0.5690\n",
      "pk: 72.68 %\n",
      "Learning rate: 0.0016650735\n",
      "Epoch: 13............. Loss: 0.7145\n",
      "pk: 73.03 %\n",
      "Learning rate: 0.0016650735\n",
      "Epoch: 14............. Loss: 0.8723\n",
      "pk: 74.86 %\n",
      "Learning rate: 0.0017316764\n",
      "Epoch: 15............. Loss: 0.6127\n",
      "pk: 74.01 %\n",
      "Learning rate: 0.0018009435\n",
      "Epoch: 16............. Loss: 0.6397\n",
      "pk: 74.58 %\n",
      "Learning rate: 0.0018009435\n",
      "Epoch: 17............. Loss: 0.8122\n",
      "pk: 74.61 %\n",
      "Learning rate: 0.0018729812\n",
      "Epoch: 18............. Loss: 0.4540\n",
      "pk: 73.45 %\n",
      "Learning rate: 0.0018729812\n",
      "Epoch: 19............. Loss: 0.6781\n",
      "pk: 73.43 %\n",
      "Learning rate: 0.0019479005\n",
      "Epoch: 20............. Loss: 0.7271\n",
      "pk: 73.14 %\n",
      "Learning rate: 0.0019479005\n",
      "Epoch: 21............. Loss: 0.6358\n",
      "pk: 74.77 %\n",
      "Learning rate: 0.0020258165\n",
      "Epoch: 22............. Loss: 0.4500\n",
      "pk: 73.19 %\n",
      "Learning rate: 0.0020258165\n",
      "Epoch: 23............. Loss: 0.6440\n",
      "pk: 74.16 %\n",
      "Learning rate: 0.0021068492\n",
      "Epoch: 24............. Loss: 0.5986\n",
      "pk: 74.37 %\n",
      "Learning rate: 0.0021068492\n",
      "Epoch: 25............. Loss: 0.7969\n",
      "pk: 75.86 %\n",
      "Learning rate: 0.0021911231\n",
      "Epoch: 26............. Loss: 0.5326\n",
      "pk: 75.07 %\n",
      "Learning rate: 0.0021911231\n",
      "Epoch: 27............. Loss: 0.5761\n",
      "pk: 73.78 %\n",
      "Learning rate: 0.0022787681\n",
      "Epoch: 28............. Loss: 0.6268\n",
      "pk: 75.62 %\n",
      "Learning rate: 0.0022787681\n",
      "Epoch: 29............. Loss: 0.5474\n",
      "pk: 74.88 %\n",
      "Learning rate: 0.0023699188\n",
      "Epoch: 30............. Loss: 0.9123\n",
      "pk: 74.68 %\n",
      "Learning rate: 0.0023699188\n",
      "Epoch: 31............. Loss: 0.6402\n",
      "pk: 75.80 %\n",
      "Learning rate: 0.0023699188\n",
      "Epoch: 32............. Loss: 0.7614\n",
      "pk: 73.89 %\n",
      "Learning rate: 0.0024647155\n",
      "Epoch: 33............. Loss: 0.5466\n",
      "pk: 75.52 %\n",
      "Learning rate: 0.0024647155\n",
      "Epoch: 34............. Loss: 0.5589\n",
      "pk: 73.47 %\n",
      "Learning rate: 0.0024647155\n",
      "Epoch: 35............. Loss: 0.7305\n",
      "pk: 74.70 %\n",
      "Learning rate: 0.0024647155\n",
      "Epoch: 36............. Loss: 0.7988\n",
      "pk: 76.14 %\n",
      "Learning rate: 0.0025633042\n",
      "Epoch: 37............. Loss: 0.6106\n",
      "pk: 75.77 %\n",
      "Learning rate: 0.0026658363\n",
      "Epoch: 38............. Loss: 0.5692\n",
      "pk: 74.36 %\n",
      "Learning rate: 0.0026658363\n",
      "Epoch: 39............. Loss: 0.7075\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "class num: 10\n",
      "ilość klas: 10\n",
      "wielkość po warstawach conv: 5\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv2d-1     [1, 16, 32, 32]           1,216           1,216\n",
      "       MaxPool2d-2     [1, 16, 17, 17]               0               0\n",
      "          Conv2d-3     [1, 32, 17, 17]          12,832          12,832\n",
      "          Conv2d-4     [1, 64, 17, 17]          51,264          51,264\n",
      "          Conv2d-5     [1, 86, 17, 17]         137,686         137,686\n",
      "       MaxPool2d-6       [1, 86, 9, 9]               0               0\n",
      "          Conv2d-7      [1, 128, 9, 9]         275,328         275,328\n",
      "          Conv2d-8      [1, 128, 9, 9]         409,728         409,728\n",
      "          Conv2d-9      [1, 128, 9, 9]         409,728         409,728\n",
      "      MaxPool2d-10      [1, 128, 5, 5]               0               0\n",
      "         Conv2d-11      [1, 128, 5, 5]         409,728         409,728\n",
      "         Linear-12             [1, 64]         204,864         204,864\n",
      "         Linear-13             [1, 16]           1,040           1,040\n",
      "         Linear-14             [1, 10]             170             170\n",
      "=======================================================================\n",
      "Total params: 1,913,584\n",
      "Trainable params: 1,913,584\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "GPU is available\n",
      "pk: 41.45 %\n",
      "Learning rate: 0.0010000000\n",
      "Epoch: 0............. Loss: 1.5010\n",
      "pk: 53.95 %\n",
      "Learning rate: 0.0010400000\n",
      "Epoch: 1............. Loss: 1.2134\n",
      "pk: 60.91 %\n",
      "Learning rate: 0.0010816000\n",
      "Epoch: 2............. Loss: 1.0013\n",
      "pk: 63.57 %\n",
      "Learning rate: 0.0011248640\n",
      "Epoch: 3............. Loss: 0.9494\n",
      "pk: 65.81 %\n",
      "Learning rate: 0.0011698586\n",
      "Epoch: 4............. Loss: 0.9854\n",
      "pk: 68.24 %\n",
      "Learning rate: 0.0012166529\n",
      "Epoch: 5............. Loss: 1.0298\n",
      "pk: 69.06 %\n",
      "Learning rate: 0.0012653190\n",
      "Epoch: 6............. Loss: 0.6941\n",
      "pk: 69.70 %\n",
      "Learning rate: 0.0013159318\n",
      "Epoch: 7............. Loss: 0.8760\n",
      "pk: 72.32 %\n",
      "Learning rate: 0.0013685691\n",
      "Epoch: 8............. Loss: 0.9353\n",
      "pk: 72.13 %\n",
      "Learning rate: 0.0014233118\n",
      "Epoch: 9............. Loss: 0.7421\n",
      "pk: 73.63 %\n",
      "Learning rate: 0.0014802443\n",
      "Epoch: 10............. Loss: 0.8575\n",
      "pk: 74.24 %\n",
      "Learning rate: 0.0015394541\n",
      "Epoch: 11............. Loss: 0.7067\n",
      "pk: 71.94 %\n",
      "Learning rate: 0.0016010322\n",
      "Epoch: 12............. Loss: 0.6443\n",
      "pk: 73.56 %\n",
      "Learning rate: 0.0016650735\n",
      "Epoch: 13............. Loss: 0.6956\n",
      "pk: 71.08 %\n",
      "Learning rate: 0.0017316764\n",
      "Epoch: 14............. Loss: 0.6261\n",
      "pk: 74.02 %\n",
      "Learning rate: 0.0018009435\n",
      "Epoch: 15............. Loss: 0.7987\n",
      "pk: 73.94 %\n",
      "Learning rate: 0.0018729812\n",
      "Epoch: 16............. Loss: 0.5901\n",
      "pk: 74.35 %\n",
      "Learning rate: 0.0018729812\n",
      "Epoch: 17............. Loss: 0.7546\n",
      "pk: 73.06 %\n",
      "Learning rate: 0.0019479005\n",
      "Epoch: 18............. Loss: 0.6598\n",
      "pk: 74.74 %\n",
      "Learning rate: 0.0020258165\n",
      "Epoch: 19............. Loss: 0.6668\n",
      "pk: 74.17 %\n",
      "Learning rate: 0.0020258165\n",
      "Epoch: 20............. Loss: 0.5622\n",
      "pk: 74.94 %\n",
      "Learning rate: 0.0021068492\n",
      "Epoch: 21............. Loss: 0.6990\n",
      "pk: 75.36 %\n",
      "Learning rate: 0.0021068492\n",
      "Epoch: 22............. Loss: 0.5988\n",
      "pk: 73.40 %\n",
      "Learning rate: 0.0021911231\n",
      "Epoch: 23............. Loss: 0.6727\n",
      "pk: 74.83 %\n",
      "Learning rate: 0.0021911231\n",
      "Epoch: 24............. Loss: 0.8381\n",
      "pk: 75.31 %\n",
      "Learning rate: 0.0022787681\n",
      "Epoch: 25............. Loss: 0.5366\n",
      "pk: 74.55 %\n",
      "Learning rate: 0.0022787681\n",
      "Epoch: 26............. Loss: 0.7150\n",
      "pk: 73.93 %\n",
      "Learning rate: 0.0023699188\n",
      "Epoch: 27............. Loss: 0.7592\n",
      "pk: 74.46 %\n",
      "Learning rate: 0.0023699188\n",
      "Epoch: 28............. Loss: 0.6041\n",
      "pk: 74.77 %\n",
      "Learning rate: 0.0023699188\n",
      "Epoch: 29............. Loss: 0.8426\n",
      "pk: 75.84 %\n",
      "Learning rate: 0.0024647155\n",
      "Epoch: 30............. Loss: 0.6318\n",
      "pk: 75.05 %\n",
      "Learning rate: 0.0024647155\n",
      "Epoch: 31............. Loss: 0.6716\n",
      "pk: 75.21 %\n",
      "Learning rate: 0.0024647155\n",
      "Epoch: 32............. Loss: 0.5165\n",
      "pk: 75.58 %\n",
      "Learning rate: 0.0025633042\n",
      "Epoch: 33............. Loss: 0.5093\n",
      "pk: 75.40 %\n",
      "Learning rate: 0.0025633042\n",
      "Epoch: 34............. Loss: 0.4932\n",
      "pk: 74.52 %\n",
      "Learning rate: 0.0025633042\n",
      "Epoch: 35............. Loss: 0.8502\n",
      "pk: 73.60 %\n",
      "Learning rate: 0.0026658363\n",
      "Epoch: 36............. Loss: 0.7322\n",
      "pk: 75.02 %\n",
      "Learning rate: 0.0026658363\n",
      "Epoch: 37............. Loss: 0.9361\n",
      "pk: 75.07 %\n",
      "Learning rate: 0.0026658363\n",
      "Epoch: 38............. Loss: 0.5540\n",
      "pk: 74.16 %\n",
      "Learning rate: 0.0027724698\n",
      "Epoch: 39............. Loss: 0.6093\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "class num: 10\n",
      "ilość klas: 10\n",
      "wielkość po warstawach conv: 5\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv2d-1     [1, 16, 32, 32]           1,216           1,216\n",
      "       MaxPool2d-2     [1, 16, 17, 17]               0               0\n",
      "          Conv2d-3     [1, 32, 17, 17]          12,832          12,832\n",
      "          Conv2d-4     [1, 64, 17, 17]          51,264          51,264\n",
      "          Conv2d-5     [1, 86, 17, 17]         137,686         137,686\n",
      "       MaxPool2d-6       [1, 86, 9, 9]               0               0\n",
      "          Conv2d-7      [1, 128, 9, 9]         275,328         275,328\n",
      "          Conv2d-8      [1, 128, 9, 9]         409,728         409,728\n",
      "          Conv2d-9      [1, 128, 9, 9]         409,728         409,728\n",
      "      MaxPool2d-10      [1, 128, 5, 5]               0               0\n",
      "         Conv2d-11      [1, 128, 5, 5]         409,728         409,728\n",
      "         Linear-12             [1, 64]         204,864         204,864\n",
      "         Linear-13             [1, 16]           1,040           1,040\n",
      "         Linear-14             [1, 10]             170             170\n",
      "=======================================================================\n",
      "Total params: 1,913,584\n",
      "Trainable params: 1,913,584\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "GPU is available\n",
      "pk: 37.36 %\n",
      "Learning rate: 0.0010000000\n",
      "Epoch: 0............. Loss: 1.6528\n",
      "pk: 51.88 %\n",
      "Learning rate: 0.0010400000\n",
      "Epoch: 1............. Loss: 1.3138\n",
      "pk: 59.71 %\n",
      "Learning rate: 0.0010816000\n",
      "Epoch: 2............. Loss: 1.0204\n",
      "pk: 64.02 %\n",
      "Learning rate: 0.0011248640\n",
      "Epoch: 3............. Loss: 0.9651\n",
      "pk: 68.18 %\n",
      "Learning rate: 0.0011698586\n",
      "Epoch: 4............. Loss: 1.0645\n",
      "pk: 69.46 %\n",
      "Learning rate: 0.0012166529\n",
      "Epoch: 5............. Loss: 0.9260\n",
      "pk: 70.89 %\n",
      "Learning rate: 0.0012653190\n",
      "Epoch: 6............. Loss: 0.9617\n",
      "pk: 71.05 %\n",
      "Learning rate: 0.0013159318\n",
      "Epoch: 7............. Loss: 0.9673\n",
      "pk: 72.21 %\n",
      "Learning rate: 0.0013685691\n",
      "Epoch: 8............. Loss: 0.8236\n",
      "pk: 72.44 %\n",
      "Learning rate: 0.0014233118\n",
      "Epoch: 9............. Loss: 0.9407\n",
      "pk: 74.11 %\n",
      "Learning rate: 0.0014802443\n",
      "Epoch: 10............. Loss: 0.8245\n",
      "pk: 72.31 %\n",
      "Learning rate: 0.0015394541\n",
      "Epoch: 11............. Loss: 0.6631\n",
      "pk: 74.08 %\n",
      "Learning rate: 0.0016010322\n",
      "Epoch: 12............. Loss: 0.8820\n",
      "pk: 74.23 %\n",
      "Learning rate: 0.0016650735\n",
      "Epoch: 13............. Loss: 0.6399\n",
      "pk: 75.50 %\n",
      "Learning rate: 0.0017316764\n",
      "Epoch: 14............. Loss: 0.6508\n",
      "pk: 74.81 %\n",
      "Learning rate: 0.0018009435\n",
      "Epoch: 15............. Loss: 0.8016\n",
      "pk: 75.00 %\n",
      "Learning rate: 0.0018729812\n",
      "Epoch: 16............. Loss: 0.7115\n",
      "pk: 75.47 %\n",
      "Learning rate: 0.0018729812\n",
      "Epoch: 17............. Loss: 0.5500\n",
      "pk: 75.38 %\n",
      "Learning rate: 0.0019479005\n",
      "Epoch: 18............. Loss: 0.5957\n",
      "pk: 75.18 %\n",
      "Learning rate: 0.0020258165\n",
      "Epoch: 19............. Loss: 0.8025\n",
      "pk: 74.49 %\n",
      "Learning rate: 0.0020258165\n",
      "Epoch: 20............. Loss: 0.6018\n",
      "pk: 74.06 %\n",
      "Learning rate: 0.0021068492\n",
      "Epoch: 21............. Loss: 0.7069\n",
      "pk: 75.37 %\n",
      "Learning rate: 0.0021068492\n",
      "Epoch: 22............. Loss: 0.7203\n",
      "pk: 74.71 %\n",
      "Learning rate: 0.0021911231\n",
      "Epoch: 23............. Loss: 0.6996\n",
      "pk: 75.37 %\n",
      "Learning rate: 0.0022787681\n",
      "Epoch: 24............. Loss: 0.7937\n",
      "pk: 74.61 %\n",
      "Learning rate: 0.0022787681\n",
      "Epoch: 25............. Loss: 0.8290\n",
      "pk: 75.87 %\n",
      "Learning rate: 0.0023699188\n",
      "Epoch: 26............. Loss: 0.7744\n",
      "pk: 73.86 %\n",
      "Learning rate: 0.0023699188\n",
      "Epoch: 27............. Loss: 0.7086\n",
      "pk: 74.84 %\n",
      "Learning rate: 0.0023699188\n",
      "Epoch: 28............. Loss: 0.6229\n",
      "pk: 75.51 %\n",
      "Learning rate: 0.0024647155\n",
      "Epoch: 29............. Loss: 0.8259\n",
      "pk: 75.72 %\n",
      "Learning rate: 0.0025633042\n",
      "Epoch: 30............. Loss: 0.5752\n",
      "pk: 75.29 %\n",
      "Learning rate: 0.0025633042\n",
      "Epoch: 31............. Loss: 0.6108\n",
      "pk: 74.29 %\n",
      "Learning rate: 0.0026658363\n",
      "Epoch: 32............. Loss: 0.8677\n",
      "pk: 73.74 %\n",
      "Learning rate: 0.0026658363\n",
      "Epoch: 33............. Loss: 0.7938\n",
      "pk: 73.96 %\n",
      "Learning rate: 0.0026658363\n",
      "Epoch: 34............. Loss: 0.8292\n",
      "pk: 74.77 %\n",
      "Learning rate: 0.0027724698\n",
      "Epoch: 35............. Loss: 0.7315\n",
      "pk: 74.89 %\n",
      "Learning rate: 0.0027724698\n",
      "Epoch: 36............. Loss: 0.7996\n",
      "pk: 73.60 %\n",
      "Learning rate: 0.0028833686\n",
      "Epoch: 37............. Loss: 0.6945\n",
      "pk: 73.12 %\n",
      "Learning rate: 0.0028833686\n",
      "Epoch: 38............. Loss: 0.6657\n",
      "pk: 75.03 %\n",
      "Learning rate: 0.0029987033\n",
      "Epoch: 39............. Loss: 0.7332\n"
     ]
    }
   ],
   "source": [
    "# input image size in px (square image)\n",
    "input_size = 32\n",
    "dataset_name = \"cifar\"\n",
    "epoch = 40\n",
    "0\n",
    "\n",
    "# methods = [\"xavier_uniform\",'xavier_uniform_M_10', 'xavier_uniform_M_2', 'xavier_uniform_M_1', 'xavier_uniform_M_14', 'xavier_uniform_M_20']\n",
    "methods = ['kaiming_uniform']\n",
    "for method in methods:\n",
    "    for apt in range(3):\n",
    "        model = TrainModel(dataset=CifarDataset(), method=method, input_size=input_size,\n",
    "                                c_kernels=[5, 5, 5, 5, 5, 5, 5, 5, 5], in_channels=[3, 16, 32, 64, 86, 128, 128, 128], out_channels=[16, 32, 64, 86, 128, 128, 128, 128], apt=apt, dataset_name=dataset_name, epoch=epoch, activation_relu=True)\n",
    "        sse, sse_t, acc, e = model.training()\n",
    "        np.savetxt(\"./output_data/data_plots/\" + dataset_name + method + \"_\" + str(apt) + \".csv\", sse, delimiter=\";\")\n",
    "        np.savetxt(\"./output_data/data_plots/\" + dataset_name + method + \"_\" + str(apt) + \"_t.csv\", sse_t, delimiter=\";\")\n",
    "        with open('./output_data/acc/' + dataset_name + method + \"_\" + str(apt) + '.txt', 'w') as f:\n",
    "            f.write('pk: ' + str(acc) + '\\n')\n",
    "        torch.save(model.cnn_model, \"./output_data/models/\" + dataset_name + method + \"_\" + str(apt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "class num: 10\n",
      "ilość klas: 10\n",
      "wielkość po warstawach conv: 9\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv2d-1     [1, 16, 32, 32]           1,216           1,216\n",
      "       MaxPool2d-2     [1, 16, 17, 17]               0               0\n",
      "          Conv2d-3     [1, 32, 17, 17]          12,832          12,832\n",
      "          Conv2d-4     [1, 64, 17, 17]          51,264          51,264\n",
      "          Conv2d-5     [1, 86, 17, 17]         137,686         137,686\n",
      "       MaxPool2d-6       [1, 86, 9, 9]               0               0\n",
      "          Conv2d-7      [1, 128, 9, 9]         275,328         275,328\n",
      "          Linear-8             [1, 64]         663,616         663,616\n",
      "          Linear-9             [1, 16]           1,040           1,040\n",
      "         Linear-10             [1, 10]             170             170\n",
      "=======================================================================\n",
      "Total params: 1,143,152\n",
      "Trainable params: 1,143,152\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "GPU is available\n",
      "pk: 10.69 %\n",
      "Learning rate: 0.0000100000\n",
      "Epoch: 0............. Loss: 2.3132\n",
      "pk: 11.75 %\n",
      "Learning rate: 0.0000104000\n",
      "Epoch: 1............. Loss: 2.2843\n",
      "pk: 13.42 %\n",
      "Learning rate: 0.0000108160\n",
      "Epoch: 2............. Loss: 2.2952\n",
      "pk: 14.45 %\n",
      "Learning rate: 0.0000112486\n",
      "Epoch: 3............. Loss: 2.2677\n",
      "pk: 15.32 %\n",
      "Learning rate: 0.0000116986\n",
      "Epoch: 4............. Loss: 2.2686\n",
      "pk: 15.89 %\n",
      "Learning rate: 0.0000121665\n",
      "Epoch: 5............. Loss: 2.2941\n",
      "pk: 16.17 %\n",
      "Learning rate: 0.0000126532\n",
      "Epoch: 6............. Loss: 2.3150\n",
      "pk: 16.58 %\n",
      "Learning rate: 0.0000131593\n",
      "Epoch: 7............. Loss: 2.2934\n",
      "pk: 17.44 %\n",
      "Learning rate: 0.0000136857\n",
      "Epoch: 8............. Loss: 2.2684\n",
      "pk: 17.88 %\n",
      "Learning rate: 0.0000142331\n",
      "Epoch: 9............. Loss: 2.2528\n",
      "pk: 18.29 %\n",
      "Learning rate: 0.0000148024\n",
      "Epoch: 10............. Loss: 2.2937\n",
      "pk: 19.25 %\n",
      "Learning rate: 0.0000153945\n",
      "Epoch: 11............. Loss: 2.2066\n",
      "pk: 19.26 %\n",
      "Learning rate: 0.0000160103\n",
      "Epoch: 12............. Loss: 2.2365\n",
      "pk: 19.91 %\n",
      "Learning rate: 0.0000166507\n",
      "Epoch: 13............. Loss: 2.1756\n",
      "pk: 20.59 %\n",
      "Learning rate: 0.0000173168\n",
      "Epoch: 14............. Loss: 2.2044\n",
      "pk: 21.98 %\n",
      "Learning rate: 0.0000180094\n",
      "Epoch: 15............. Loss: 2.1140\n",
      "pk: 24.66 %\n",
      "Learning rate: 0.0000187298\n",
      "Epoch: 16............. Loss: 2.1950\n",
      "pk: 25.90 %\n",
      "Learning rate: 0.0000194790\n",
      "Epoch: 17............. Loss: 2.0894\n",
      "pk: 27.13 %\n",
      "Learning rate: 0.0000202582\n",
      "Epoch: 18............. Loss: 2.0647\n",
      "pk: 29.41 %\n",
      "Learning rate: 0.0000210685\n",
      "Epoch: 19............. Loss: 1.9582\n",
      "pk: 29.93 %\n",
      "Learning rate: 0.0000219112\n",
      "Epoch: 20............. Loss: 2.0380\n",
      "pk: 32.15 %\n",
      "Learning rate: 0.0000227877\n",
      "Epoch: 21............. Loss: 1.9169\n",
      "pk: 32.60 %\n",
      "Learning rate: 0.0000236992\n",
      "Epoch: 22............. Loss: 1.9387\n",
      "pk: 34.28 %\n",
      "Learning rate: 0.0000246472\n",
      "Epoch: 23............. Loss: 1.8932\n",
      "pk: 35.43 %\n",
      "Learning rate: 0.0000256330\n",
      "Epoch: 24............. Loss: 2.0709\n",
      "pk: 36.65 %\n",
      "Learning rate: 0.0000266584\n",
      "Epoch: 25............. Loss: 1.8908\n",
      "pk: 38.54 %\n",
      "Learning rate: 0.0000277247\n",
      "Epoch: 26............. Loss: 1.7969\n",
      "pk: 39.82 %\n",
      "Learning rate: 0.0000288337\n",
      "Epoch: 27............. Loss: 1.8001\n",
      "pk: 40.52 %\n",
      "Learning rate: 0.0000299870\n",
      "Epoch: 28............. Loss: 1.7822\n",
      "pk: 42.46 %\n",
      "Learning rate: 0.0000311865\n",
      "Epoch: 29............. Loss: 1.7293\n",
      "pk: 43.13 %\n",
      "Learning rate: 0.0000324340\n",
      "Epoch: 30............. Loss: 1.7752\n",
      "pk: 43.45 %\n",
      "Learning rate: 0.0000337313\n",
      "Epoch: 31............. Loss: 1.7071\n",
      "pk: 44.62 %\n",
      "Learning rate: 0.0000350806\n",
      "Epoch: 32............. Loss: 1.7081\n",
      "pk: 45.00 %\n",
      "Learning rate: 0.0000364838\n",
      "Epoch: 33............. Loss: 1.7860\n",
      "pk: 46.12 %\n",
      "Learning rate: 0.0000379432\n",
      "Epoch: 34............. Loss: 1.6785\n",
      "pk: 46.14 %\n",
      "Learning rate: 0.0000394609\n",
      "Epoch: 35............. Loss: 1.5173\n",
      "pk: 46.77 %\n",
      "Learning rate: 0.0000410393\n",
      "Epoch: 36............. Loss: 1.4927\n",
      "pk: 47.35 %\n",
      "Learning rate: 0.0000426809\n",
      "Epoch: 37............. Loss: 1.6401\n",
      "pk: 48.08 %\n",
      "Learning rate: 0.0000443881\n",
      "Epoch: 38............. Loss: 1.4195\n",
      "pk: 48.69 %\n",
      "Learning rate: 0.0000461637\n",
      "Epoch: 39............. Loss: 1.5281\n",
      "pk: 49.44 %\n",
      "Learning rate: 0.0000480102\n",
      "Epoch: 40............. Loss: 1.6527\n",
      "pk: 50.18 %\n",
      "Learning rate: 0.0000499306\n",
      "Epoch: 41............. Loss: 1.5616\n",
      "pk: 49.86 %\n",
      "Learning rate: 0.0000519278\n",
      "Epoch: 42............. Loss: 1.4496\n",
      "pk: 50.97 %\n",
      "Learning rate: 0.0000540050\n",
      "Epoch: 43............. Loss: 1.2590\n",
      "pk: 51.41 %\n",
      "Learning rate: 0.0000561652\n",
      "Epoch: 44............. Loss: 1.5862\n",
      "pk: 51.24 %\n",
      "Learning rate: 0.0000584118\n",
      "Epoch: 45............. Loss: 1.4296\n",
      "pk: 52.83 %\n",
      "Learning rate: 0.0000607482\n",
      "Epoch: 46............. Loss: 1.3775\n",
      "pk: 52.47 %\n",
      "Learning rate: 0.0000631782\n",
      "Epoch: 47............. Loss: 1.4622\n",
      "pk: 53.41 %\n",
      "Learning rate: 0.0000657053\n",
      "Epoch: 48............. Loss: 1.3411\n",
      "pk: 53.96 %\n",
      "Learning rate: 0.0000683335\n",
      "Epoch: 49............. Loss: 1.4369\n",
      "pk: 54.00 %\n",
      "Learning rate: 0.0000710668\n",
      "Epoch: 50............. Loss: 1.4062\n",
      "pk: 54.25 %\n",
      "Learning rate: 0.0000739095\n",
      "Epoch: 51............. Loss: 1.2963\n",
      "pk: 55.32 %\n",
      "Learning rate: 0.0000768659\n",
      "Epoch: 52............. Loss: 1.2910\n",
      "pk: 56.10 %\n",
      "Learning rate: 0.0000799405\n",
      "Epoch: 53............. Loss: 1.4428\n",
      "pk: 56.14 %\n",
      "Learning rate: 0.0000831381\n",
      "Epoch: 54............. Loss: 1.2958\n",
      "pk: 57.66 %\n",
      "Learning rate: 0.0000864637\n",
      "Epoch: 55............. Loss: 1.1551\n",
      "pk: 58.62 %\n",
      "Learning rate: 0.0000899222\n",
      "Epoch: 56............. Loss: 1.3841\n",
      "pk: 58.83 %\n",
      "Learning rate: 0.0000935191\n",
      "Epoch: 57............. Loss: 1.1983\n",
      "pk: 58.25 %\n",
      "Learning rate: 0.0000972599\n",
      "Epoch: 58............. Loss: 1.4064\n",
      "pk: 59.98 %\n",
      "Learning rate: 0.0001011503\n",
      "Epoch: 59............. Loss: 1.1981\n",
      "pk: 58.81 %\n",
      "Learning rate: 0.0001051963\n",
      "Epoch: 60............. Loss: 1.0906\n",
      "pk: 60.36 %\n",
      "Learning rate: 0.0001094041\n",
      "Epoch: 61............. Loss: 1.0655\n",
      "pk: 60.29 %\n",
      "Learning rate: 0.0001137803\n",
      "Epoch: 62............. Loss: 1.3069\n",
      "pk: 61.78 %\n",
      "Learning rate: 0.0001183315\n",
      "Epoch: 63............. Loss: 0.9315\n",
      "pk: 61.51 %\n",
      "Learning rate: 0.0001230648\n",
      "Epoch: 64............. Loss: 1.0232\n",
      "pk: 62.77 %\n",
      "Learning rate: 0.0001279874\n",
      "Epoch: 65............. Loss: 1.0514\n",
      "pk: 62.23 %\n",
      "Learning rate: 0.0001331068\n",
      "Epoch: 66............. Loss: 1.1510\n",
      "pk: 62.08 %\n",
      "Learning rate: 0.0001384311\n",
      "Epoch: 67............. Loss: 1.2078\n",
      "pk: 64.32 %\n",
      "Learning rate: 0.0001439684\n",
      "Epoch: 68............. Loss: 1.2856\n",
      "pk: 63.95 %\n",
      "Learning rate: 0.0001497271\n",
      "Epoch: 69............. Loss: 0.9545\n",
      "pk: 64.79 %\n",
      "Learning rate: 0.0001557162\n",
      "Epoch: 70............. Loss: 1.0075\n",
      "pk: 65.17 %\n",
      "Learning rate: 0.0001619448\n",
      "Epoch: 71............. Loss: 1.0791\n",
      "pk: 65.99 %\n",
      "Learning rate: 0.0001684226\n",
      "Epoch: 72............. Loss: 1.0112\n",
      "pk: 65.85 %\n",
      "Learning rate: 0.0001751595\n",
      "Epoch: 73............. Loss: 0.8734\n",
      "pk: 66.78 %\n",
      "Learning rate: 0.0001821659\n",
      "Epoch: 74............. Loss: 1.0286\n",
      "pk: 64.95 %\n",
      "Learning rate: 0.0001894525\n",
      "Epoch: 75............. Loss: 1.0611\n",
      "pk: 66.52 %\n",
      "Learning rate: 0.0001970306\n",
      "Epoch: 76............. Loss: 0.9316\n",
      "pk: 67.49 %\n",
      "Learning rate: 0.0002049119\n",
      "Epoch: 77............. Loss: 0.8632\n",
      "pk: 67.16 %\n",
      "Learning rate: 0.0002131083\n",
      "Epoch: 78............. Loss: 0.8554\n",
      "pk: 67.19 %\n",
      "Learning rate: 0.0002216327\n",
      "Epoch: 79............. Loss: 0.9862\n",
      "pk: 68.44 %\n",
      "Learning rate: 0.0002304980\n",
      "Epoch: 80............. Loss: 1.0208\n",
      "pk: 69.29 %\n",
      "Learning rate: 0.0002397179\n",
      "Epoch: 81............. Loss: 0.6909\n",
      "pk: 69.21 %\n",
      "Learning rate: 0.0002493066\n",
      "Epoch: 82............. Loss: 0.8283\n",
      "pk: 68.24 %\n",
      "Learning rate: 0.0002592789\n",
      "Epoch: 83............. Loss: 0.9672\n",
      "pk: 68.34 %\n",
      "Learning rate: 0.0002696500\n",
      "Epoch: 84............. Loss: 0.7534\n",
      "pk: 69.09 %\n",
      "Learning rate: 0.0002804360\n",
      "Epoch: 85............. Loss: 0.8148\n",
      "pk: 69.94 %\n",
      "Learning rate: 0.0002916535\n",
      "Epoch: 86............. Loss: 0.8022\n",
      "pk: 70.07 %\n",
      "Learning rate: 0.0003033196\n",
      "Epoch: 87............. Loss: 0.6538\n",
      "pk: 70.66 %\n",
      "Learning rate: 0.0003154524\n",
      "Epoch: 88............. Loss: 0.7314\n",
      "pk: 69.67 %\n",
      "Learning rate: 0.0003280705\n",
      "Epoch: 89............. Loss: 0.6740\n",
      "pk: 69.89 %\n",
      "Learning rate: 0.0003411933\n",
      "Epoch: 90............. Loss: 0.6915\n",
      "pk: 71.08 %\n",
      "Learning rate: 0.0003548411\n",
      "Epoch: 91............. Loss: 0.6186\n",
      "pk: 70.67 %\n",
      "Learning rate: 0.0003690347\n",
      "Epoch: 92............. Loss: 0.7426\n",
      "pk: 70.62 %\n",
      "Learning rate: 0.0003837961\n",
      "Epoch: 93............. Loss: 0.5437\n",
      "pk: 71.29 %\n",
      "Learning rate: 0.0003991479\n",
      "Epoch: 94............. Loss: 0.5689\n",
      "pk: 71.40 %\n",
      "Learning rate: 0.0004151139\n",
      "Epoch: 95............. Loss: 0.4972\n",
      "pk: 72.10 %\n",
      "Learning rate: 0.0004317184\n",
      "Epoch: 96............. Loss: 0.5943\n",
      "pk: 71.90 %\n",
      "Learning rate: 0.0004489872\n",
      "Epoch: 97............. Loss: 0.6484\n",
      "pk: 71.26 %\n",
      "Learning rate: 0.0004669466\n",
      "Epoch: 98............. Loss: 0.5248\n",
      "pk: 72.38 %\n",
      "Learning rate: 0.0004856245\n",
      "Epoch: 99............. Loss: 0.5817\n",
      "pk: 71.95 %\n",
      "Learning rate: 0.0005050495\n",
      "Epoch: 100............. Loss: 0.3769\n",
      "pk: 72.30 %\n",
      "Learning rate: 0.0005252515\n",
      "Epoch: 101............. Loss: 0.4259\n",
      "pk: 72.59 %\n",
      "Learning rate: 0.0005462615\n",
      "Epoch: 102............. Loss: 0.6512\n",
      "pk: 72.52 %\n",
      "Learning rate: 0.0005681120\n",
      "Epoch: 103............. Loss: 0.6686\n",
      "pk: 72.13 %\n",
      "Learning rate: 0.0005908365\n",
      "Epoch: 104............. Loss: 0.4760\n",
      "pk: 72.70 %\n",
      "Learning rate: 0.0006144699\n",
      "Epoch: 105............. Loss: 0.3277\n",
      "pk: 72.92 %\n",
      "Learning rate: 0.0006390487\n",
      "Epoch: 106............. Loss: 0.5748\n",
      "pk: 72.34 %\n",
      "Learning rate: 0.0006646107\n",
      "Epoch: 107............. Loss: 0.4479\n",
      "pk: 72.48 %\n",
      "Learning rate: 0.0006911951\n",
      "Epoch: 108............. Loss: 0.4340\n",
      "pk: 73.71 %\n",
      "Learning rate: 0.0007188429\n",
      "Epoch: 109............. Loss: 0.5861\n",
      "pk: 72.56 %\n",
      "Learning rate: 0.0007475966\n",
      "Epoch: 110............. Loss: 0.3147\n",
      "pk: 72.63 %\n",
      "Learning rate: 0.0007775005\n",
      "Epoch: 111............. Loss: 0.5251\n",
      "pk: 73.11 %\n",
      "Learning rate: 0.0008086005\n",
      "Epoch: 112............. Loss: 0.5520\n",
      "pk: 72.90 %\n",
      "Learning rate: 0.0008409445\n",
      "Epoch: 113............. Loss: 0.4385\n",
      "pk: 73.93 %\n",
      "Learning rate: 0.0008745823\n",
      "Epoch: 114............. Loss: 0.2979\n",
      "pk: 73.52 %\n",
      "Learning rate: 0.0009095656\n",
      "Epoch: 115............. Loss: 0.3350\n",
      "pk: 72.58 %\n",
      "Learning rate: 0.0009459482\n",
      "Epoch: 116............. Loss: 0.3834\n",
      "pk: 73.48 %\n",
      "Learning rate: 0.0009837861\n",
      "Epoch: 117............. Loss: 0.1805\n",
      "pk: 72.73 %\n",
      "Learning rate: 0.0010231376\n",
      "Epoch: 118............. Loss: 0.2826\n",
      "pk: 72.14 %\n",
      "Learning rate: 0.0010640631\n",
      "Epoch: 119............. Loss: 0.3928\n",
      "pk: 72.51 %\n",
      "Learning rate: 0.0011066256\n",
      "Epoch: 120............. Loss: 0.3356\n",
      "pk: 72.88 %\n",
      "Learning rate: 0.0011508906\n",
      "Epoch: 121............. Loss: 0.2984\n",
      "pk: 73.81 %\n",
      "Learning rate: 0.0011969263\n",
      "Epoch: 122............. Loss: 0.3225\n",
      "pk: 71.85 %\n",
      "Learning rate: 0.0012448033\n",
      "Epoch: 123............. Loss: 0.1802\n",
      "pk: 73.28 %\n",
      "Learning rate: 0.0012945954\n",
      "Epoch: 124............. Loss: 0.2465\n",
      "pk: 72.54 %\n",
      "Learning rate: 0.0013463793\n",
      "Epoch: 125............. Loss: 0.3533\n",
      "pk: 71.74 %\n",
      "Learning rate: 0.0014002344\n",
      "Epoch: 126............. Loss: 0.4417\n",
      "pk: 73.13 %\n",
      "Learning rate: 0.0014002344\n",
      "Epoch: 127............. Loss: 0.1244\n",
      "pk: 73.67 %\n",
      "Learning rate: 0.0014562438\n",
      "Epoch: 128............. Loss: 0.3270\n",
      "pk: 73.77 %\n",
      "Learning rate: 0.0015144936\n",
      "Epoch: 129............. Loss: 0.2951\n",
      "pk: 72.94 %\n",
      "Learning rate: 0.0015144936\n",
      "Epoch: 130............. Loss: 0.3545\n",
      "pk: 73.17 %\n",
      "Learning rate: 0.0015750733\n",
      "Epoch: 131............. Loss: 0.1999\n",
      "pk: 73.42 %\n",
      "Learning rate: 0.0016380762\n",
      "Epoch: 132............. Loss: 0.4791\n",
      "pk: 73.59 %\n",
      "Learning rate: 0.0017035993\n",
      "Epoch: 133............. Loss: 0.1825\n",
      "pk: 73.53 %\n",
      "Learning rate: 0.0017717433\n",
      "Epoch: 134............. Loss: 0.3004\n",
      "pk: 73.42 %\n",
      "Learning rate: 0.0017717433\n",
      "Epoch: 135............. Loss: 0.2506\n",
      "pk: 73.27 %\n",
      "Learning rate: 0.0018426130\n",
      "Epoch: 136............. Loss: 0.1456\n",
      "pk: 73.92 %\n",
      "Learning rate: 0.0019163175\n",
      "Epoch: 137............. Loss: 0.3143\n",
      "pk: 72.48 %\n",
      "Learning rate: 0.0019929702\n",
      "Epoch: 138............. Loss: 0.1213\n",
      "pk: 72.24 %\n",
      "Learning rate: 0.0019929702\n",
      "Epoch: 139............. Loss: 0.2820\n",
      "pk: 73.21 %\n",
      "Learning rate: 0.0020726890\n",
      "Epoch: 140............. Loss: 0.2912\n",
      "pk: 73.14 %\n",
      "Learning rate: 0.0021555966\n",
      "Epoch: 141............. Loss: 0.1268\n",
      "pk: 74.36 %\n",
      "Learning rate: 0.0021555966\n",
      "Epoch: 142............. Loss: 0.2136\n",
      "pk: 72.64 %\n",
      "Learning rate: 0.0022418204\n",
      "Epoch: 143............. Loss: 0.2642\n",
      "pk: 73.11 %\n",
      "Learning rate: 0.0023314933\n",
      "Epoch: 144............. Loss: 0.2234\n",
      "pk: 73.32 %\n",
      "Learning rate: 0.0023314933\n",
      "Epoch: 145............. Loss: 0.1122\n",
      "pk: 73.32 %\n",
      "Learning rate: 0.0024247530\n",
      "Epoch: 146............. Loss: 0.1053\n",
      "pk: 72.36 %\n",
      "Learning rate: 0.0024247530\n",
      "Epoch: 147............. Loss: 0.1750\n",
      "pk: 73.65 %\n",
      "Learning rate: 0.0025217431\n",
      "Epoch: 148............. Loss: 0.2684\n",
      "pk: 73.39 %\n",
      "Learning rate: 0.0025217431\n",
      "Epoch: 149............. Loss: 0.2215\n",
      "pk: 73.85 %\n",
      "Learning rate: 0.0026226128\n",
      "Epoch: 150............. Loss: 0.1762\n",
      "pk: 73.88 %\n",
      "Learning rate: 0.0026226128\n",
      "Epoch: 151............. Loss: 0.1475\n",
      "pk: 73.44 %\n",
      "Learning rate: 0.0027275173\n",
      "Epoch: 152............. Loss: 0.2085\n",
      "pk: 72.47 %\n",
      "Learning rate: 0.0028366180\n",
      "Epoch: 153............. Loss: 0.1131\n",
      "pk: 72.20 %\n",
      "Learning rate: 0.0029500828\n",
      "Epoch: 154............. Loss: 0.2215\n",
      "pk: 73.56 %\n",
      "Learning rate: 0.0029500828\n",
      "Epoch: 155............. Loss: 0.2111\n",
      "pk: 73.18 %\n",
      "Learning rate: 0.0030680861\n",
      "Epoch: 156............. Loss: 0.1798\n",
      "pk: 73.80 %\n",
      "Learning rate: 0.0031908095\n",
      "Epoch: 157............. Loss: 0.1108\n",
      "pk: 72.79 %\n",
      "Learning rate: 0.0031908095\n",
      "Epoch: 158............. Loss: 0.1583\n",
      "pk: 74.17 %\n",
      "Learning rate: 0.0033184419\n",
      "Epoch: 159............. Loss: 0.2123\n",
      "pk: 73.55 %\n",
      "Learning rate: 0.0034511796\n",
      "Epoch: 160............. Loss: 0.1417\n",
      "pk: 73.66 %\n",
      "Learning rate: 0.0034511796\n",
      "Epoch: 161............. Loss: 0.0945\n",
      "pk: 72.20 %\n",
      "Learning rate: 0.0035892267\n",
      "Epoch: 162............. Loss: 0.2863\n",
      "pk: 73.11 %\n",
      "Learning rate: 0.0035892267\n",
      "Epoch: 163............. Loss: 0.0949\n",
      "pk: 73.04 %\n",
      "Learning rate: 0.0037327958\n",
      "Epoch: 164............. Loss: 0.1700\n",
      "pk: 72.04 %\n",
      "Learning rate: 0.0037327958\n",
      "Epoch: 165............. Loss: 0.2807\n",
      "pk: 73.36 %\n",
      "Learning rate: 0.0038821076\n",
      "Epoch: 166............. Loss: 0.2284\n",
      "pk: 73.07 %\n",
      "Learning rate: 0.0038821076\n",
      "Epoch: 167............. Loss: 0.1939\n",
      "pk: 74.03 %\n",
      "Learning rate: 0.0040373919\n",
      "Epoch: 168............. Loss: 0.1524\n",
      "pk: 72.92 %\n",
      "Learning rate: 0.0040373919\n",
      "Epoch: 169............. Loss: 0.2813\n",
      "pk: 73.90 %\n",
      "Learning rate: 0.0041988876\n",
      "Epoch: 170............. Loss: 0.1780\n",
      "pk: 73.01 %\n",
      "Learning rate: 0.0041988876\n",
      "Epoch: 171............. Loss: 0.1627\n",
      "pk: 73.17 %\n",
      "Learning rate: 0.0043668431\n",
      "Epoch: 172............. Loss: 0.1359\n",
      "pk: 73.68 %\n",
      "Learning rate: 0.0043668431\n",
      "Epoch: 173............. Loss: 0.2216\n",
      "pk: 73.01 %\n",
      "Learning rate: 0.0045415169\n",
      "Epoch: 174............. Loss: 0.1265\n",
      "pk: 72.79 %\n",
      "Learning rate: 0.0045415169\n",
      "Epoch: 175............. Loss: 0.1523\n",
      "pk: 73.83 %\n",
      "Learning rate: 0.0047231775\n",
      "Epoch: 176............. Loss: 0.1185\n",
      "pk: 73.22 %\n",
      "Learning rate: 0.0047231775\n",
      "Epoch: 177............. Loss: 0.0733\n",
      "pk: 73.99 %\n",
      "Learning rate: 0.0047231775\n",
      "Epoch: 178............. Loss: 0.1766\n",
      "pk: 73.90 %\n",
      "Learning rate: 0.0049121046\n",
      "Epoch: 179............. Loss: 0.2234\n",
      "pk: 73.42 %\n",
      "Learning rate: 0.0049121046\n",
      "Epoch: 180............. Loss: 0.2271\n",
      "pk: 71.89 %\n",
      "Learning rate: 0.0051085888\n",
      "Epoch: 181............. Loss: 0.2416\n",
      "pk: 72.38 %\n",
      "Learning rate: 0.0051085888\n",
      "Epoch: 182............. Loss: 0.2065\n",
      "pk: 74.23 %\n",
      "Learning rate: 0.0053129324\n",
      "Epoch: 183............. Loss: 0.0945\n",
      "pk: 74.10 %\n",
      "Learning rate: 0.0055254497\n",
      "Epoch: 184............. Loss: 0.1131\n",
      "pk: 72.38 %\n",
      "Learning rate: 0.0055254497\n",
      "Epoch: 185............. Loss: 0.1526\n",
      "pk: 73.49 %\n",
      "Learning rate: 0.0057464677\n",
      "Epoch: 186............. Loss: 0.1335\n",
      "pk: 74.13 %\n",
      "Learning rate: 0.0057464677\n",
      "Epoch: 187............. Loss: 0.2262\n",
      "pk: 72.04 %\n",
      "Learning rate: 0.0057464677\n",
      "Epoch: 188............. Loss: 0.2306\n",
      "pk: 73.74 %\n",
      "Learning rate: 0.0059763264\n",
      "Epoch: 189............. Loss: 0.2169\n",
      "pk: 72.93 %\n",
      "Learning rate: 0.0062153794\n",
      "Epoch: 190............. Loss: 0.1433\n",
      "pk: 73.37 %\n",
      "Learning rate: 0.0062153794\n",
      "Epoch: 191............. Loss: 0.2815\n",
      "pk: 73.50 %\n",
      "Learning rate: 0.0062153794\n",
      "Epoch: 192............. Loss: 0.0921\n",
      "pk: 73.36 %\n",
      "Learning rate: 0.0062153794\n",
      "Epoch: 193............. Loss: 0.2583\n",
      "pk: 73.65 %\n",
      "Learning rate: 0.0064639946\n",
      "Epoch: 194............. Loss: 0.2354\n",
      "pk: 73.50 %\n",
      "Learning rate: 0.0064639946\n",
      "Epoch: 195............. Loss: 0.2437\n",
      "pk: 72.93 %\n",
      "Learning rate: 0.0067225544\n",
      "Epoch: 196............. Loss: 0.1372\n",
      "pk: 72.99 %\n",
      "Learning rate: 0.0067225544\n",
      "Epoch: 197............. Loss: 0.2299\n",
      "pk: 72.85 %\n",
      "Learning rate: 0.0069914565\n",
      "Epoch: 198............. Loss: 0.2910\n",
      "pk: 73.09 %\n",
      "Learning rate: 0.0072711148\n",
      "Epoch: 199............. Loss: 0.1648\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "class num: 10\n",
      "ilość klas: 10\n",
      "wielkość po warstawach conv: 9\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv2d-1     [1, 16, 32, 32]           1,216           1,216\n",
      "       MaxPool2d-2     [1, 16, 17, 17]               0               0\n",
      "          Conv2d-3     [1, 32, 17, 17]          12,832          12,832\n",
      "          Conv2d-4     [1, 64, 17, 17]          51,264          51,264\n",
      "          Conv2d-5     [1, 86, 17, 17]         137,686         137,686\n",
      "       MaxPool2d-6       [1, 86, 9, 9]               0               0\n",
      "          Conv2d-7      [1, 128, 9, 9]         275,328         275,328\n",
      "          Linear-8             [1, 64]         663,616         663,616\n",
      "          Linear-9             [1, 16]           1,040           1,040\n",
      "         Linear-10             [1, 10]             170             170\n",
      "=======================================================================\n",
      "Total params: 1,143,152\n",
      "Trainable params: 1,143,152\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "GPU is available\n",
      "pk: 14.30 %\n",
      "Learning rate: 0.0000100000\n",
      "Epoch: 0............. Loss: 2.2474\n",
      "pk: 15.81 %\n",
      "Learning rate: 0.0000104000\n",
      "Epoch: 1............. Loss: 2.2215\n",
      "pk: 17.52 %\n",
      "Learning rate: 0.0000108160\n",
      "Epoch: 2............. Loss: 2.3052\n",
      "pk: 18.99 %\n",
      "Learning rate: 0.0000112486\n",
      "Epoch: 3............. Loss: 2.1852\n",
      "pk: 21.29 %\n",
      "Learning rate: 0.0000116986\n",
      "Epoch: 4............. Loss: 2.1501\n",
      "pk: 22.80 %\n",
      "Learning rate: 0.0000121665\n",
      "Epoch: 5............. Loss: 2.1358\n",
      "pk: 24.55 %\n",
      "Learning rate: 0.0000126532\n",
      "Epoch: 6............. Loss: 2.1740\n",
      "pk: 26.55 %\n",
      "Learning rate: 0.0000131593\n",
      "Epoch: 7............. Loss: 2.1022\n",
      "pk: 27.52 %\n",
      "Learning rate: 0.0000136857\n",
      "Epoch: 8............. Loss: 2.1320\n",
      "pk: 28.23 %\n",
      "Learning rate: 0.0000142331\n",
      "Epoch: 9............. Loss: 2.1006\n",
      "pk: 29.27 %\n",
      "Learning rate: 0.0000148024\n",
      "Epoch: 10............. Loss: 2.1853\n",
      "pk: 30.15 %\n",
      "Learning rate: 0.0000153945\n",
      "Epoch: 11............. Loss: 2.1485\n",
      "pk: 30.34 %\n",
      "Learning rate: 0.0000160103\n",
      "Epoch: 12............. Loss: 2.0137\n",
      "pk: 30.94 %\n",
      "Learning rate: 0.0000166507\n",
      "Epoch: 13............. Loss: 2.1152\n",
      "pk: 31.64 %\n",
      "Learning rate: 0.0000173168\n",
      "Epoch: 14............. Loss: 1.9584\n",
      "pk: 32.00 %\n",
      "Learning rate: 0.0000180094\n",
      "Epoch: 15............. Loss: 2.0718\n",
      "pk: 32.54 %\n",
      "Learning rate: 0.0000187298\n",
      "Epoch: 16............. Loss: 2.0027\n",
      "pk: 32.76 %\n",
      "Learning rate: 0.0000194790\n",
      "Epoch: 17............. Loss: 2.0519\n",
      "pk: 34.41 %\n",
      "Learning rate: 0.0000202582\n",
      "Epoch: 18............. Loss: 1.9053\n",
      "pk: 35.54 %\n",
      "Learning rate: 0.0000210685\n",
      "Epoch: 19............. Loss: 1.9852\n",
      "pk: 35.91 %\n",
      "Learning rate: 0.0000219112\n",
      "Epoch: 20............. Loss: 1.8934\n",
      "pk: 36.68 %\n",
      "Learning rate: 0.0000227877\n",
      "Epoch: 21............. Loss: 1.8144\n",
      "pk: 37.79 %\n",
      "Learning rate: 0.0000236992\n",
      "Epoch: 22............. Loss: 1.9338\n",
      "pk: 38.36 %\n",
      "Learning rate: 0.0000246472\n",
      "Epoch: 23............. Loss: 1.7234\n",
      "pk: 39.80 %\n",
      "Learning rate: 0.0000256330\n",
      "Epoch: 24............. Loss: 1.7494\n",
      "pk: 41.25 %\n",
      "Learning rate: 0.0000266584\n",
      "Epoch: 25............. Loss: 1.8214\n",
      "pk: 42.38 %\n",
      "Learning rate: 0.0000277247\n",
      "Epoch: 26............. Loss: 1.7995\n",
      "pk: 42.13 %\n",
      "Learning rate: 0.0000288337\n",
      "Epoch: 27............. Loss: 1.7774\n",
      "pk: 43.37 %\n",
      "Learning rate: 0.0000299870\n",
      "Epoch: 28............. Loss: 1.6884\n",
      "pk: 43.79 %\n",
      "Learning rate: 0.0000311865\n",
      "Epoch: 29............. Loss: 1.5513\n",
      "pk: 44.72 %\n",
      "Learning rate: 0.0000324340\n",
      "Epoch: 30............. Loss: 1.5931\n",
      "pk: 45.48 %\n",
      "Learning rate: 0.0000337313\n",
      "Epoch: 31............. Loss: 1.6759\n",
      "pk: 45.79 %\n",
      "Learning rate: 0.0000350806\n",
      "Epoch: 32............. Loss: 1.6068\n",
      "pk: 45.56 %\n",
      "Learning rate: 0.0000364838\n",
      "Epoch: 33............. Loss: 1.7053\n",
      "pk: 46.68 %\n",
      "Learning rate: 0.0000379432\n",
      "Epoch: 34............. Loss: 1.5048\n",
      "pk: 47.65 %\n",
      "Learning rate: 0.0000394609\n",
      "Epoch: 35............. Loss: 1.6745\n",
      "pk: 48.05 %\n",
      "Learning rate: 0.0000410393\n",
      "Epoch: 36............. Loss: 1.6070\n",
      "pk: 49.13 %\n",
      "Learning rate: 0.0000426809\n",
      "Epoch: 37............. Loss: 1.5572\n",
      "pk: 49.61 %\n",
      "Learning rate: 0.0000443881\n",
      "Epoch: 38............. Loss: 1.3773\n",
      "pk: 50.68 %\n",
      "Learning rate: 0.0000461637\n",
      "Epoch: 39............. Loss: 1.5069\n",
      "pk: 51.26 %\n",
      "Learning rate: 0.0000480102\n",
      "Epoch: 40............. Loss: 1.6220\n",
      "pk: 50.27 %\n",
      "Learning rate: 0.0000499306\n",
      "Epoch: 41............. Loss: 1.4262\n",
      "pk: 51.85 %\n",
      "Learning rate: 0.0000519278\n",
      "Epoch: 42............. Loss: 1.3875\n",
      "pk: 51.67 %\n",
      "Learning rate: 0.0000540050\n",
      "Epoch: 43............. Loss: 1.5139\n",
      "pk: 53.41 %\n",
      "Learning rate: 0.0000561652\n",
      "Epoch: 44............. Loss: 1.4706\n",
      "pk: 53.09 %\n",
      "Learning rate: 0.0000584118\n",
      "Epoch: 45............. Loss: 1.4629\n",
      "pk: 53.29 %\n",
      "Learning rate: 0.0000607482\n",
      "Epoch: 46............. Loss: 1.2851\n",
      "pk: 54.15 %\n",
      "Learning rate: 0.0000631782\n",
      "Epoch: 47............. Loss: 1.2411\n",
      "pk: 54.63 %\n",
      "Learning rate: 0.0000657053\n",
      "Epoch: 48............. Loss: 1.3505\n",
      "pk: 55.38 %\n",
      "Learning rate: 0.0000683335\n",
      "Epoch: 49............. Loss: 1.1034\n",
      "pk: 55.63 %\n",
      "Learning rate: 0.0000710668\n",
      "Epoch: 50............. Loss: 1.5493\n",
      "pk: 56.60 %\n",
      "Learning rate: 0.0000739095\n",
      "Epoch: 51............. Loss: 1.2941\n",
      "pk: 56.96 %\n",
      "Learning rate: 0.0000768659\n",
      "Epoch: 52............. Loss: 1.2181\n",
      "pk: 57.54 %\n",
      "Learning rate: 0.0000799405\n",
      "Epoch: 53............. Loss: 1.4556\n",
      "pk: 56.35 %\n",
      "Learning rate: 0.0000831381\n",
      "Epoch: 54............. Loss: 1.1135\n",
      "pk: 58.40 %\n",
      "Learning rate: 0.0000864637\n",
      "Epoch: 55............. Loss: 1.1955\n",
      "pk: 58.89 %\n",
      "Learning rate: 0.0000899222\n",
      "Epoch: 56............. Loss: 1.3179\n",
      "pk: 58.12 %\n",
      "Learning rate: 0.0000935191\n",
      "Epoch: 57............. Loss: 1.1968\n",
      "pk: 59.44 %\n",
      "Learning rate: 0.0000972599\n",
      "Epoch: 58............. Loss: 1.2177\n",
      "pk: 59.94 %\n",
      "Learning rate: 0.0001011503\n",
      "Epoch: 59............. Loss: 1.2376\n",
      "pk: 60.85 %\n",
      "Learning rate: 0.0001051963\n",
      "Epoch: 60............. Loss: 1.1001\n",
      "pk: 60.81 %\n",
      "Learning rate: 0.0001094041\n",
      "Epoch: 61............. Loss: 1.2648\n",
      "pk: 61.50 %\n",
      "Learning rate: 0.0001137803\n",
      "Epoch: 62............. Loss: 1.1101\n",
      "pk: 61.45 %\n",
      "Learning rate: 0.0001183315\n",
      "Epoch: 63............. Loss: 1.3299\n",
      "pk: 61.52 %\n",
      "Learning rate: 0.0001230648\n",
      "Epoch: 64............. Loss: 1.3498\n",
      "pk: 61.78 %\n",
      "Learning rate: 0.0001279874\n",
      "Epoch: 65............. Loss: 1.1418\n",
      "pk: 63.49 %\n",
      "Learning rate: 0.0001331068\n",
      "Epoch: 66............. Loss: 1.0077\n",
      "pk: 62.86 %\n",
      "Learning rate: 0.0001384311\n",
      "Epoch: 67............. Loss: 1.0607\n",
      "pk: 64.06 %\n",
      "Learning rate: 0.0001439684\n",
      "Epoch: 68............. Loss: 0.8762\n",
      "pk: 64.84 %\n",
      "Learning rate: 0.0001497271\n",
      "Epoch: 69............. Loss: 0.9193\n",
      "pk: 64.41 %\n",
      "Learning rate: 0.0001557162\n",
      "Epoch: 70............. Loss: 1.2645\n",
      "pk: 65.20 %\n",
      "Learning rate: 0.0001619448\n",
      "Epoch: 71............. Loss: 0.9269\n",
      "pk: 65.16 %\n",
      "Learning rate: 0.0001684226\n",
      "Epoch: 72............. Loss: 0.9102\n",
      "pk: 66.16 %\n",
      "Learning rate: 0.0001751595\n",
      "Epoch: 73............. Loss: 0.8266\n",
      "pk: 66.37 %\n",
      "Learning rate: 0.0001821659\n",
      "Epoch: 74............. Loss: 1.1625\n",
      "pk: 67.02 %\n",
      "Learning rate: 0.0001894525\n",
      "Epoch: 75............. Loss: 0.8666\n",
      "pk: 67.36 %\n",
      "Learning rate: 0.0001970306\n",
      "Epoch: 76............. Loss: 0.9822\n",
      "pk: 66.70 %\n",
      "Learning rate: 0.0002049119\n",
      "Epoch: 77............. Loss: 0.8867\n",
      "pk: 67.32 %\n",
      "Learning rate: 0.0002131083\n",
      "Epoch: 78............. Loss: 0.9186\n",
      "pk: 66.29 %\n",
      "Learning rate: 0.0002216327\n",
      "Epoch: 79............. Loss: 0.8184\n",
      "pk: 68.93 %\n",
      "Learning rate: 0.0002304980\n",
      "Epoch: 80............. Loss: 0.9287\n",
      "pk: 68.14 %\n",
      "Learning rate: 0.0002397179\n",
      "Epoch: 81............. Loss: 0.8791\n",
      "pk: 69.40 %\n",
      "Learning rate: 0.0002493066\n",
      "Epoch: 82............. Loss: 0.8570\n",
      "pk: 69.69 %\n",
      "Learning rate: 0.0002592789\n",
      "Epoch: 83............. Loss: 0.9216\n",
      "pk: 68.86 %\n",
      "Learning rate: 0.0002696500\n",
      "Epoch: 84............. Loss: 0.8039\n",
      "pk: 69.83 %\n",
      "Learning rate: 0.0002804360\n",
      "Epoch: 85............. Loss: 0.8744\n",
      "pk: 70.90 %\n",
      "Learning rate: 0.0002916535\n",
      "Epoch: 86............. Loss: 0.9034\n",
      "pk: 69.61 %\n",
      "Learning rate: 0.0003033196\n",
      "Epoch: 87............. Loss: 0.6383\n",
      "pk: 70.72 %\n",
      "Learning rate: 0.0003154524\n",
      "Epoch: 88............. Loss: 0.7657\n",
      "pk: 69.92 %\n",
      "Learning rate: 0.0003280705\n",
      "Epoch: 89............. Loss: 0.7985\n",
      "pk: 70.70 %\n",
      "Learning rate: 0.0003411933\n",
      "Epoch: 90............. Loss: 0.6656\n",
      "pk: 71.76 %\n",
      "Learning rate: 0.0003548411\n",
      "Epoch: 91............. Loss: 0.9634\n",
      "pk: 72.15 %\n",
      "Learning rate: 0.0003690347\n",
      "Epoch: 92............. Loss: 0.5045\n",
      "pk: 71.74 %\n",
      "Learning rate: 0.0003837961\n",
      "Epoch: 93............. Loss: 0.9393\n",
      "pk: 71.46 %\n",
      "Learning rate: 0.0003991479\n",
      "Epoch: 94............. Loss: 0.6565\n",
      "pk: 72.48 %\n",
      "Learning rate: 0.0004151139\n",
      "Epoch: 95............. Loss: 0.7810\n",
      "pk: 71.42 %\n",
      "Learning rate: 0.0004317184\n",
      "Epoch: 96............. Loss: 0.6458\n",
      "pk: 72.07 %\n",
      "Learning rate: 0.0004489872\n",
      "Epoch: 97............. Loss: 0.4450\n",
      "pk: 71.50 %\n",
      "Learning rate: 0.0004669466\n",
      "Epoch: 98............. Loss: 0.6135\n",
      "pk: 72.16 %\n",
      "Learning rate: 0.0004856245\n",
      "Epoch: 99............. Loss: 0.5387\n",
      "pk: 71.59 %\n",
      "Learning rate: 0.0005050495\n",
      "Epoch: 100............. Loss: 0.7359\n",
      "pk: 72.13 %\n",
      "Learning rate: 0.0005252515\n",
      "Epoch: 101............. Loss: 0.6049\n",
      "pk: 73.84 %\n",
      "Learning rate: 0.0005462615\n",
      "Epoch: 102............. Loss: 0.5447\n",
      "pk: 72.79 %\n",
      "Learning rate: 0.0005681120\n",
      "Epoch: 103............. Loss: 0.5011\n",
      "pk: 73.22 %\n",
      "Learning rate: 0.0005908365\n",
      "Epoch: 104............. Loss: 0.4900\n",
      "pk: 72.66 %\n",
      "Learning rate: 0.0006144699\n",
      "Epoch: 105............. Loss: 0.6285\n",
      "pk: 73.23 %\n",
      "Learning rate: 0.0006390487\n",
      "Epoch: 106............. Loss: 0.4225\n",
      "pk: 72.56 %\n",
      "Learning rate: 0.0006646107\n",
      "Epoch: 107............. Loss: 0.3969\n",
      "pk: 71.93 %\n",
      "Learning rate: 0.0006911951\n",
      "Epoch: 108............. Loss: 0.4609\n",
      "pk: 72.69 %\n",
      "Learning rate: 0.0007188429\n",
      "Epoch: 109............. Loss: 0.2889\n",
      "pk: 72.94 %\n",
      "Learning rate: 0.0007475966\n",
      "Epoch: 110............. Loss: 0.4817\n",
      "pk: 73.22 %\n",
      "Learning rate: 0.0007775005\n",
      "Epoch: 111............. Loss: 0.5104\n",
      "pk: 72.70 %\n",
      "Learning rate: 0.0008086005\n",
      "Epoch: 112............. Loss: 0.4306\n",
      "pk: 71.74 %\n",
      "Learning rate: 0.0008409445\n",
      "Epoch: 113............. Loss: 0.5201\n",
      "pk: 71.45 %\n",
      "Learning rate: 0.0008745823\n",
      "Epoch: 114............. Loss: 0.6640\n",
      "pk: 73.15 %\n",
      "Learning rate: 0.0009095656\n",
      "Epoch: 115............. Loss: 0.2695\n",
      "pk: 72.63 %\n",
      "Learning rate: 0.0009459482\n",
      "Epoch: 116............. Loss: 0.4800\n",
      "pk: 71.34 %\n",
      "Learning rate: 0.0009837861\n",
      "Epoch: 117............. Loss: 0.3305\n",
      "pk: 72.46 %\n",
      "Learning rate: 0.0010231376\n",
      "Epoch: 118............. Loss: 0.4133\n",
      "pk: 72.05 %\n",
      "Learning rate: 0.0010640631\n",
      "Epoch: 119............. Loss: 0.2346\n",
      "pk: 71.54 %\n",
      "Learning rate: 0.0011066256\n",
      "Epoch: 120............. Loss: 0.4630\n",
      "pk: 72.88 %\n",
      "Learning rate: 0.0011066256\n",
      "Epoch: 121............. Loss: 0.2977\n",
      "pk: 71.84 %\n",
      "Learning rate: 0.0011508906\n",
      "Epoch: 122............. Loss: 0.2998\n",
      "pk: 71.71 %\n",
      "Learning rate: 0.0011969263\n",
      "Epoch: 123............. Loss: 0.2227\n",
      "pk: 72.81 %\n",
      "Learning rate: 0.0012448033\n",
      "Epoch: 124............. Loss: 0.2263\n",
      "pk: 72.22 %\n",
      "Learning rate: 0.0012945954\n",
      "Epoch: 125............. Loss: 0.2156\n",
      "pk: 71.18 %\n",
      "Learning rate: 0.0012945954\n",
      "Epoch: 126............. Loss: 0.2163\n",
      "pk: 73.05 %\n",
      "Learning rate: 0.0013463793\n",
      "Epoch: 127............. Loss: 0.1279\n",
      "pk: 73.40 %\n",
      "Learning rate: 0.0014002344\n",
      "Epoch: 128............. Loss: 0.2326\n",
      "pk: 72.51 %\n",
      "Learning rate: 0.0014562438\n",
      "Epoch: 129............. Loss: 0.2168\n",
      "pk: 72.80 %\n",
      "Learning rate: 0.0015144936\n",
      "Epoch: 130............. Loss: 0.2136\n",
      "pk: 70.67 %\n",
      "Learning rate: 0.0015144936\n",
      "Epoch: 131............. Loss: 0.2012\n",
      "pk: 73.00 %\n",
      "Learning rate: 0.0015750733\n",
      "Epoch: 132............. Loss: 0.1936\n",
      "pk: 72.85 %\n",
      "Learning rate: 0.0016380762\n",
      "Epoch: 133............. Loss: 0.2174\n",
      "pk: 72.66 %\n",
      "Learning rate: 0.0016380762\n",
      "Epoch: 134............. Loss: 0.2840\n",
      "pk: 72.61 %\n",
      "Learning rate: 0.0017035993\n",
      "Epoch: 135............. Loss: 0.3009\n",
      "pk: 72.93 %\n",
      "Learning rate: 0.0017717433\n",
      "Epoch: 136............. Loss: 0.1802\n",
      "pk: 72.23 %\n",
      "Learning rate: 0.0017717433\n",
      "Epoch: 137............. Loss: 0.1698\n",
      "pk: 72.37 %\n",
      "Learning rate: 0.0018426130\n",
      "Epoch: 138............. Loss: 0.2303\n",
      "pk: 71.89 %\n",
      "Learning rate: 0.0019163175\n",
      "Epoch: 139............. Loss: 0.1895\n",
      "pk: 71.37 %\n",
      "Learning rate: 0.0019163175\n",
      "Epoch: 140............. Loss: 0.1019\n",
      "pk: 72.31 %\n",
      "Learning rate: 0.0019929702\n",
      "Epoch: 141............. Loss: 0.2279\n",
      "pk: 71.69 %\n",
      "Learning rate: 0.0020726890\n",
      "Epoch: 142............. Loss: 0.2594\n",
      "pk: 72.62 %\n",
      "Learning rate: 0.0021555966\n",
      "Epoch: 143............. Loss: 0.2193\n",
      "pk: 73.32 %\n",
      "Learning rate: 0.0021555966\n",
      "Epoch: 144............. Loss: 0.1576\n",
      "pk: 72.11 %\n",
      "Learning rate: 0.0022418204\n",
      "Epoch: 145............. Loss: 0.1459\n",
      "pk: 71.49 %\n",
      "Learning rate: 0.0022418204\n",
      "Epoch: 146............. Loss: 0.2046\n",
      "pk: 72.26 %\n",
      "Learning rate: 0.0023314933\n",
      "Epoch: 147............. Loss: 0.2042\n",
      "pk: 72.36 %\n",
      "Learning rate: 0.0023314933\n",
      "Epoch: 148............. Loss: 0.1414\n",
      "pk: 71.70 %\n",
      "Learning rate: 0.0024247530\n",
      "Epoch: 149............. Loss: 0.3915\n",
      "pk: 73.15 %\n",
      "Learning rate: 0.0025217431\n",
      "Epoch: 150............. Loss: 0.1133\n",
      "pk: 71.83 %\n",
      "Learning rate: 0.0025217431\n",
      "Epoch: 151............. Loss: 0.2436\n",
      "pk: 73.24 %\n",
      "Learning rate: 0.0026226128\n",
      "Epoch: 152............. Loss: 0.0810\n",
      "pk: 71.76 %\n",
      "Learning rate: 0.0026226128\n",
      "Epoch: 153............. Loss: 0.2624\n",
      "pk: 72.21 %\n",
      "Learning rate: 0.0027275173\n",
      "Epoch: 154............. Loss: 0.1745\n",
      "pk: 72.40 %\n",
      "Learning rate: 0.0027275173\n",
      "Epoch: 155............. Loss: 0.2118\n",
      "pk: 70.89 %\n",
      "Learning rate: 0.0028366180\n",
      "Epoch: 156............. Loss: 0.1360\n",
      "pk: 72.98 %\n",
      "Learning rate: 0.0029500828\n",
      "Epoch: 157............. Loss: 0.1276\n",
      "pk: 73.30 %\n",
      "Learning rate: 0.0029500828\n",
      "Epoch: 158............. Loss: 0.1929\n",
      "pk: 72.56 %\n",
      "Learning rate: 0.0030680861\n",
      "Epoch: 159............. Loss: 0.1974\n",
      "pk: 72.63 %\n",
      "Learning rate: 0.0030680861\n",
      "Epoch: 160............. Loss: 0.2086\n",
      "pk: 73.07 %\n",
      "Learning rate: 0.0031908095\n",
      "Epoch: 161............. Loss: 0.2429\n",
      "pk: 72.05 %\n",
      "Learning rate: 0.0031908095\n",
      "Epoch: 162............. Loss: 0.1438\n",
      "pk: 72.06 %\n",
      "Learning rate: 0.0031908095\n",
      "Epoch: 163............. Loss: 0.1233\n",
      "pk: 72.53 %\n",
      "Learning rate: 0.0031908095\n",
      "Epoch: 164............. Loss: 0.1126\n",
      "pk: 72.94 %\n",
      "Learning rate: 0.0033184419\n",
      "Epoch: 165............. Loss: 0.2180\n",
      "pk: 71.78 %\n",
      "Learning rate: 0.0034511796\n",
      "Epoch: 166............. Loss: 0.1255\n",
      "pk: 71.65 %\n",
      "Learning rate: 0.0034511796\n",
      "Epoch: 167............. Loss: 0.1919\n",
      "pk: 72.61 %\n",
      "Learning rate: 0.0034511796\n",
      "Epoch: 168............. Loss: 0.2223\n",
      "pk: 72.48 %\n",
      "Learning rate: 0.0034511796\n",
      "Epoch: 169............. Loss: 0.1934\n",
      "pk: 72.22 %\n",
      "Learning rate: 0.0035892267\n",
      "Epoch: 170............. Loss: 0.1692\n",
      "pk: 72.91 %\n",
      "Learning rate: 0.0035892267\n",
      "Epoch: 171............. Loss: 0.1744\n",
      "pk: 73.07 %\n",
      "Learning rate: 0.0037327958\n",
      "Epoch: 172............. Loss: 0.0950\n",
      "pk: 73.50 %\n",
      "Learning rate: 0.0037327958\n",
      "Epoch: 173............. Loss: 0.1540\n",
      "pk: 73.45 %\n",
      "Learning rate: 0.0038821076\n",
      "Epoch: 174............. Loss: 0.1744\n",
      "pk: 72.24 %\n",
      "Learning rate: 0.0038821076\n",
      "Epoch: 175............. Loss: 0.1454\n",
      "pk: 73.41 %\n",
      "Learning rate: 0.0040373919\n",
      "Epoch: 176............. Loss: 0.0671\n",
      "pk: 72.11 %\n",
      "Learning rate: 0.0040373919\n",
      "Epoch: 177............. Loss: 0.0622\n",
      "pk: 72.25 %\n",
      "Learning rate: 0.0041988876\n",
      "Epoch: 178............. Loss: 0.1496\n",
      "pk: 72.92 %\n",
      "Learning rate: 0.0041988876\n",
      "Epoch: 179............. Loss: 0.1000\n",
      "pk: 72.72 %\n",
      "Learning rate: 0.0043668431\n",
      "Epoch: 180............. Loss: 0.2831\n",
      "pk: 72.41 %\n",
      "Learning rate: 0.0043668431\n",
      "Epoch: 181............. Loss: 0.2477\n",
      "pk: 73.34 %\n",
      "Learning rate: 0.0045415169\n",
      "Epoch: 182............. Loss: 0.1432\n",
      "pk: 72.75 %\n",
      "Learning rate: 0.0045415169\n",
      "Epoch: 183............. Loss: 0.2038\n",
      "pk: 72.93 %\n",
      "Learning rate: 0.0047231775\n",
      "Epoch: 184............. Loss: 0.1737\n",
      "pk: 72.92 %\n",
      "Learning rate: 0.0047231775\n",
      "Epoch: 185............. Loss: 0.2101\n",
      "pk: 71.97 %\n",
      "Learning rate: 0.0049121046\n",
      "Epoch: 186............. Loss: 0.1026\n",
      "pk: 73.05 %\n",
      "Learning rate: 0.0049121046\n",
      "Epoch: 187............. Loss: 0.2415\n",
      "pk: 72.91 %\n",
      "Learning rate: 0.0051085888\n",
      "Epoch: 188............. Loss: 0.2770\n",
      "pk: 72.46 %\n",
      "Learning rate: 0.0051085888\n",
      "Epoch: 189............. Loss: 0.1601\n",
      "pk: 72.55 %\n",
      "Learning rate: 0.0053129324\n",
      "Epoch: 190............. Loss: 0.1641\n",
      "pk: 72.01 %\n",
      "Learning rate: 0.0053129324\n",
      "Epoch: 191............. Loss: 0.2046\n",
      "pk: 72.34 %\n",
      "Learning rate: 0.0053129324\n",
      "Epoch: 192............. Loss: 0.2855\n",
      "pk: 72.35 %\n",
      "Learning rate: 0.0055254497\n",
      "Epoch: 193............. Loss: 0.1019\n",
      "pk: 71.56 %\n",
      "Learning rate: 0.0055254497\n",
      "Epoch: 194............. Loss: 0.0952\n",
      "pk: 71.34 %\n",
      "Learning rate: 0.0055254497\n",
      "Epoch: 195............. Loss: 0.1661\n",
      "pk: 71.51 %\n",
      "Learning rate: 0.0057464677\n",
      "Epoch: 196............. Loss: 0.1346\n",
      "pk: 71.35 %\n",
      "Learning rate: 0.0057464677\n",
      "Epoch: 197............. Loss: 0.3553\n",
      "pk: 72.66 %\n",
      "Learning rate: 0.0059763264\n",
      "Epoch: 198............. Loss: 0.2152\n",
      "pk: 72.60 %\n",
      "Learning rate: 0.0059763264\n",
      "Epoch: 199............. Loss: 0.1491\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "class num: 10\n",
      "ilość klas: 10\n",
      "wielkość po warstawach conv: 9\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv2d-1     [1, 16, 32, 32]           1,216           1,216\n",
      "       MaxPool2d-2     [1, 16, 17, 17]               0               0\n",
      "          Conv2d-3     [1, 32, 17, 17]          12,832          12,832\n",
      "          Conv2d-4     [1, 64, 17, 17]          51,264          51,264\n",
      "          Conv2d-5     [1, 86, 17, 17]         137,686         137,686\n",
      "       MaxPool2d-6       [1, 86, 9, 9]               0               0\n",
      "          Conv2d-7      [1, 128, 9, 9]         275,328         275,328\n",
      "          Linear-8             [1, 64]         663,616         663,616\n",
      "          Linear-9             [1, 16]           1,040           1,040\n",
      "         Linear-10             [1, 10]             170             170\n",
      "=======================================================================\n",
      "Total params: 1,143,152\n",
      "Trainable params: 1,143,152\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "GPU is available\n",
      "pk: 12.37 %\n",
      "Learning rate: 0.0000100000\n",
      "Epoch: 0............. Loss: 2.3008\n",
      "pk: 14.67 %\n",
      "Learning rate: 0.0000104000\n",
      "Epoch: 1............. Loss: 2.2950\n",
      "pk: 17.95 %\n",
      "Learning rate: 0.0000108160\n",
      "Epoch: 2............. Loss: 2.2716\n",
      "pk: 20.79 %\n",
      "Learning rate: 0.0000112486\n",
      "Epoch: 3............. Loss: 2.2929\n",
      "pk: 21.65 %\n",
      "Learning rate: 0.0000116986\n",
      "Epoch: 4............. Loss: 2.2328\n",
      "pk: 22.51 %\n",
      "Learning rate: 0.0000121665\n",
      "Epoch: 5............. Loss: 2.2412\n",
      "pk: 22.77 %\n",
      "Learning rate: 0.0000126532\n",
      "Epoch: 6............. Loss: 2.1959\n",
      "pk: 23.19 %\n",
      "Learning rate: 0.0000131593\n",
      "Epoch: 7............. Loss: 2.1553\n",
      "pk: 24.04 %\n",
      "Learning rate: 0.0000136857\n",
      "Epoch: 8............. Loss: 2.2096\n",
      "pk: 24.74 %\n",
      "Learning rate: 0.0000142331\n",
      "Epoch: 9............. Loss: 2.0520\n",
      "pk: 25.41 %\n",
      "Learning rate: 0.0000148024\n",
      "Epoch: 10............. Loss: 2.1978\n",
      "pk: 27.08 %\n",
      "Learning rate: 0.0000153945\n",
      "Epoch: 11............. Loss: 2.0640\n",
      "pk: 27.09 %\n",
      "Learning rate: 0.0000160103\n",
      "Epoch: 12............. Loss: 2.0405\n",
      "pk: 30.21 %\n",
      "Learning rate: 0.0000166507\n",
      "Epoch: 13............. Loss: 2.0529\n",
      "pk: 31.78 %\n",
      "Learning rate: 0.0000173168\n",
      "Epoch: 14............. Loss: 2.0075\n",
      "pk: 33.38 %\n",
      "Learning rate: 0.0000180094\n",
      "Epoch: 15............. Loss: 1.9655\n",
      "pk: 34.05 %\n",
      "Learning rate: 0.0000187298\n",
      "Epoch: 16............. Loss: 1.9043\n",
      "pk: 34.76 %\n",
      "Learning rate: 0.0000194790\n",
      "Epoch: 17............. Loss: 1.9152\n",
      "pk: 35.11 %\n",
      "Learning rate: 0.0000202582\n",
      "Epoch: 18............. Loss: 1.8049\n",
      "pk: 36.46 %\n",
      "Learning rate: 0.0000210685\n",
      "Epoch: 19............. Loss: 1.8033\n",
      "pk: 37.13 %\n",
      "Learning rate: 0.0000219112\n",
      "Epoch: 20............. Loss: 1.9653\n",
      "pk: 37.44 %\n",
      "Learning rate: 0.0000227877\n",
      "Epoch: 21............. Loss: 1.6381\n",
      "pk: 39.20 %\n",
      "Learning rate: 0.0000236992\n",
      "Epoch: 22............. Loss: 1.7418\n",
      "pk: 39.82 %\n",
      "Learning rate: 0.0000246472\n",
      "Epoch: 23............. Loss: 1.6123\n",
      "pk: 40.74 %\n",
      "Learning rate: 0.0000256330\n",
      "Epoch: 24............. Loss: 1.7367\n",
      "pk: 41.13 %\n",
      "Learning rate: 0.0000266584\n",
      "Epoch: 25............. Loss: 1.7071\n",
      "pk: 41.41 %\n",
      "Learning rate: 0.0000277247\n",
      "Epoch: 26............. Loss: 1.7061\n",
      "pk: 42.42 %\n",
      "Learning rate: 0.0000288337\n",
      "Epoch: 27............. Loss: 1.6977\n",
      "pk: 42.65 %\n",
      "Learning rate: 0.0000299870\n",
      "Epoch: 28............. Loss: 1.5388\n",
      "pk: 42.95 %\n",
      "Learning rate: 0.0000311865\n",
      "Epoch: 29............. Loss: 1.5876\n",
      "pk: 44.30 %\n",
      "Learning rate: 0.0000324340\n",
      "Epoch: 30............. Loss: 1.6972\n",
      "pk: 45.13 %\n",
      "Learning rate: 0.0000337313\n",
      "Epoch: 31............. Loss: 1.5275\n",
      "pk: 44.62 %\n",
      "Learning rate: 0.0000350806\n",
      "Epoch: 32............. Loss: 1.5534\n",
      "pk: 46.31 %\n",
      "Learning rate: 0.0000364838\n",
      "Epoch: 33............. Loss: 1.5985\n",
      "pk: 46.37 %\n",
      "Learning rate: 0.0000379432\n",
      "Epoch: 34............. Loss: 1.6027\n",
      "pk: 47.07 %\n",
      "Learning rate: 0.0000394609\n",
      "Epoch: 35............. Loss: 1.4899\n",
      "pk: 46.44 %\n",
      "Learning rate: 0.0000410393\n",
      "Epoch: 36............. Loss: 1.4703\n",
      "pk: 47.45 %\n",
      "Learning rate: 0.0000426809\n",
      "Epoch: 37............. Loss: 1.5039\n",
      "pk: 48.98 %\n",
      "Learning rate: 0.0000443881\n",
      "Epoch: 38............. Loss: 1.5788\n",
      "pk: 49.18 %\n",
      "Learning rate: 0.0000461637\n",
      "Epoch: 39............. Loss: 1.3950\n",
      "pk: 49.53 %\n",
      "Learning rate: 0.0000480102\n",
      "Epoch: 40............. Loss: 1.5085\n",
      "pk: 51.00 %\n",
      "Learning rate: 0.0000499306\n",
      "Epoch: 41............. Loss: 1.2969\n",
      "pk: 50.66 %\n",
      "Learning rate: 0.0000519278\n",
      "Epoch: 42............. Loss: 1.4859\n",
      "pk: 51.71 %\n",
      "Learning rate: 0.0000540050\n",
      "Epoch: 43............. Loss: 1.2319\n",
      "pk: 52.35 %\n",
      "Learning rate: 0.0000561652\n",
      "Epoch: 44............. Loss: 1.4152\n",
      "pk: 53.06 %\n",
      "Learning rate: 0.0000584118\n",
      "Epoch: 45............. Loss: 1.3150\n",
      "pk: 52.69 %\n",
      "Learning rate: 0.0000607482\n",
      "Epoch: 46............. Loss: 1.3487\n",
      "pk: 53.81 %\n",
      "Learning rate: 0.0000631782\n",
      "Epoch: 47............. Loss: 1.4639\n",
      "pk: 54.53 %\n",
      "Learning rate: 0.0000657053\n",
      "Epoch: 48............. Loss: 1.5316\n",
      "pk: 55.40 %\n",
      "Learning rate: 0.0000683335\n",
      "Epoch: 49............. Loss: 1.4084\n",
      "pk: 55.75 %\n",
      "Learning rate: 0.0000710668\n",
      "Epoch: 50............. Loss: 1.2469\n",
      "pk: 55.88 %\n",
      "Learning rate: 0.0000739095\n",
      "Epoch: 51............. Loss: 1.2612\n",
      "pk: 56.71 %\n",
      "Learning rate: 0.0000768659\n",
      "Epoch: 52............. Loss: 1.2511\n",
      "pk: 56.87 %\n",
      "Learning rate: 0.0000799405\n",
      "Epoch: 53............. Loss: 1.3556\n",
      "pk: 55.87 %\n",
      "Learning rate: 0.0000831381\n",
      "Epoch: 54............. Loss: 1.6732\n",
      "pk: 57.48 %\n",
      "Learning rate: 0.0000864637\n",
      "Epoch: 55............. Loss: 1.1541\n",
      "pk: 58.36 %\n",
      "Learning rate: 0.0000899222\n",
      "Epoch: 56............. Loss: 0.9791\n",
      "pk: 56.95 %\n",
      "Learning rate: 0.0000935191\n",
      "Epoch: 57............. Loss: 1.2321\n",
      "pk: 58.52 %\n",
      "Learning rate: 0.0000972599\n",
      "Epoch: 58............. Loss: 1.2738\n",
      "pk: 59.63 %\n",
      "Learning rate: 0.0001011503\n",
      "Epoch: 59............. Loss: 1.3526\n",
      "pk: 60.32 %\n",
      "Learning rate: 0.0001051963\n",
      "Epoch: 60............. Loss: 1.0858\n",
      "pk: 60.78 %\n",
      "Learning rate: 0.0001094041\n",
      "Epoch: 61............. Loss: 1.1421\n",
      "pk: 61.50 %\n",
      "Learning rate: 0.0001137803\n",
      "Epoch: 62............. Loss: 1.1209\n",
      "pk: 62.14 %\n",
      "Learning rate: 0.0001183315\n",
      "Epoch: 63............. Loss: 1.0526\n",
      "pk: 61.16 %\n",
      "Learning rate: 0.0001230648\n",
      "Epoch: 64............. Loss: 1.1317\n",
      "pk: 61.88 %\n",
      "Learning rate: 0.0001279874\n",
      "Epoch: 65............. Loss: 1.0189\n",
      "pk: 62.06 %\n",
      "Learning rate: 0.0001331068\n",
      "Epoch: 66............. Loss: 1.2254\n",
      "pk: 63.33 %\n",
      "Learning rate: 0.0001384311\n",
      "Epoch: 67............. Loss: 1.0854\n",
      "pk: 63.35 %\n",
      "Learning rate: 0.0001439684\n",
      "Epoch: 68............. Loss: 1.2876\n",
      "pk: 64.02 %\n",
      "Learning rate: 0.0001497271\n",
      "Epoch: 69............. Loss: 0.8969\n",
      "pk: 62.16 %\n",
      "Learning rate: 0.0001557162\n",
      "Epoch: 70............. Loss: 1.1455\n",
      "pk: 63.12 %\n",
      "Learning rate: 0.0001619448\n",
      "Epoch: 71............. Loss: 1.0135\n",
      "pk: 64.34 %\n",
      "Learning rate: 0.0001684226\n",
      "Epoch: 72............. Loss: 1.1204\n",
      "pk: 64.35 %\n",
      "Learning rate: 0.0001751595\n",
      "Epoch: 73............. Loss: 0.8596\n",
      "pk: 65.73 %\n",
      "Learning rate: 0.0001821659\n",
      "Epoch: 74............. Loss: 0.8156\n",
      "pk: 65.94 %\n",
      "Learning rate: 0.0001894525\n",
      "Epoch: 75............. Loss: 0.9399\n",
      "pk: 65.13 %\n",
      "Learning rate: 0.0001970306\n",
      "Epoch: 76............. Loss: 0.9524\n",
      "pk: 66.08 %\n",
      "Learning rate: 0.0002049119\n",
      "Epoch: 77............. Loss: 1.0250\n",
      "pk: 67.62 %\n",
      "Learning rate: 0.0002131083\n",
      "Epoch: 78............. Loss: 0.7843\n",
      "pk: 67.38 %\n",
      "Learning rate: 0.0002216327\n",
      "Epoch: 79............. Loss: 0.8447\n",
      "pk: 68.24 %\n",
      "Learning rate: 0.0002304980\n",
      "Epoch: 80............. Loss: 0.7318\n",
      "pk: 68.06 %\n",
      "Learning rate: 0.0002397179\n",
      "Epoch: 81............. Loss: 0.8525\n",
      "pk: 68.62 %\n",
      "Learning rate: 0.0002493066\n",
      "Epoch: 82............. Loss: 0.9588\n",
      "pk: 68.06 %\n",
      "Learning rate: 0.0002592789\n",
      "Epoch: 83............. Loss: 0.9446\n",
      "pk: 69.03 %\n",
      "Learning rate: 0.0002696500\n",
      "Epoch: 84............. Loss: 0.8165\n",
      "pk: 69.70 %\n",
      "Learning rate: 0.0002804360\n",
      "Epoch: 85............. Loss: 0.8885\n",
      "pk: 69.71 %\n",
      "Learning rate: 0.0002916535\n",
      "Epoch: 86............. Loss: 0.7574\n",
      "pk: 69.24 %\n",
      "Learning rate: 0.0003033196\n",
      "Epoch: 87............. Loss: 0.8036\n",
      "pk: 70.44 %\n",
      "Learning rate: 0.0003154524\n",
      "Epoch: 88............. Loss: 0.7621\n",
      "pk: 70.13 %\n",
      "Learning rate: 0.0003280705\n",
      "Epoch: 89............. Loss: 0.6839\n",
      "pk: 70.66 %\n",
      "Learning rate: 0.0003411933\n",
      "Epoch: 90............. Loss: 0.5868\n",
      "pk: 70.24 %\n",
      "Learning rate: 0.0003548411\n",
      "Epoch: 91............. Loss: 0.6578\n",
      "pk: 71.16 %\n",
      "Learning rate: 0.0003690347\n",
      "Epoch: 92............. Loss: 0.6072\n",
      "pk: 71.30 %\n",
      "Learning rate: 0.0003837961\n",
      "Epoch: 93............. Loss: 0.6593\n",
      "pk: 71.11 %\n",
      "Learning rate: 0.0003991479\n",
      "Epoch: 94............. Loss: 0.6498\n",
      "pk: 70.25 %\n",
      "Learning rate: 0.0004151139\n",
      "Epoch: 95............. Loss: 0.5843\n",
      "pk: 71.92 %\n",
      "Learning rate: 0.0004317184\n",
      "Epoch: 96............. Loss: 0.6891\n",
      "pk: 71.95 %\n",
      "Learning rate: 0.0004489872\n",
      "Epoch: 97............. Loss: 0.5306\n",
      "pk: 71.73 %\n",
      "Learning rate: 0.0004669466\n",
      "Epoch: 98............. Loss: 0.5668\n",
      "pk: 71.72 %\n",
      "Learning rate: 0.0004856245\n",
      "Epoch: 99............. Loss: 0.7167\n",
      "pk: 70.72 %\n",
      "Learning rate: 0.0005050495\n",
      "Epoch: 100............. Loss: 0.5013\n",
      "pk: 71.47 %\n",
      "Learning rate: 0.0005252515\n",
      "Epoch: 101............. Loss: 0.4901\n",
      "pk: 71.80 %\n",
      "Learning rate: 0.0005462615\n",
      "Epoch: 102............. Loss: 0.5480\n",
      "pk: 72.29 %\n",
      "Learning rate: 0.0005681120\n",
      "Epoch: 103............. Loss: 0.4611\n",
      "pk: 72.37 %\n",
      "Learning rate: 0.0005908365\n",
      "Epoch: 104............. Loss: 0.5779\n",
      "pk: 72.22 %\n",
      "Learning rate: 0.0006144699\n",
      "Epoch: 105............. Loss: 0.5421\n",
      "pk: 70.76 %\n",
      "Learning rate: 0.0006390487\n",
      "Epoch: 106............. Loss: 0.6183\n",
      "pk: 72.47 %\n",
      "Learning rate: 0.0006646107\n",
      "Epoch: 107............. Loss: 0.5342\n",
      "pk: 72.32 %\n",
      "Learning rate: 0.0006911951\n",
      "Epoch: 108............. Loss: 0.4931\n",
      "pk: 72.86 %\n",
      "Learning rate: 0.0007188429\n",
      "Epoch: 109............. Loss: 0.4551\n",
      "pk: 72.81 %\n",
      "Learning rate: 0.0007475966\n",
      "Epoch: 110............. Loss: 0.4484\n",
      "pk: 72.65 %\n",
      "Learning rate: 0.0007775005\n",
      "Epoch: 111............. Loss: 0.4840\n",
      "pk: 73.08 %\n",
      "Learning rate: 0.0008086005\n",
      "Epoch: 112............. Loss: 0.5008\n",
      "pk: 72.97 %\n",
      "Learning rate: 0.0008409445\n",
      "Epoch: 113............. Loss: 0.4474\n",
      "pk: 72.14 %\n",
      "Learning rate: 0.0008745823\n",
      "Epoch: 114............. Loss: 0.2951\n",
      "pk: 72.93 %\n",
      "Learning rate: 0.0009095656\n",
      "Epoch: 115............. Loss: 0.3956\n",
      "pk: 73.43 %\n",
      "Learning rate: 0.0009459482\n",
      "Epoch: 116............. Loss: 0.3840\n",
      "pk: 70.85 %\n",
      "Learning rate: 0.0009837861\n",
      "Epoch: 117............. Loss: 0.5009\n",
      "pk: 72.60 %\n",
      "Learning rate: 0.0010231376\n",
      "Epoch: 118............. Loss: 0.3680\n",
      "pk: 71.42 %\n",
      "Learning rate: 0.0010640631\n",
      "Epoch: 119............. Loss: 0.3796\n",
      "pk: 72.52 %\n",
      "Learning rate: 0.0010640631\n",
      "Epoch: 120............. Loss: 0.2850\n",
      "pk: 72.12 %\n",
      "Learning rate: 0.0011066256\n",
      "Epoch: 121............. Loss: 0.4620\n",
      "pk: 72.55 %\n",
      "Learning rate: 0.0011508906\n",
      "Epoch: 122............. Loss: 0.2558\n",
      "pk: 72.12 %\n",
      "Learning rate: 0.0011508906\n",
      "Epoch: 123............. Loss: 0.2636\n",
      "pk: 72.80 %\n",
      "Learning rate: 0.0011969263\n",
      "Epoch: 124............. Loss: 0.3771\n",
      "pk: 73.58 %\n",
      "Learning rate: 0.0012448033\n",
      "Epoch: 125............. Loss: 0.2833\n",
      "pk: 73.23 %\n",
      "Learning rate: 0.0012945954\n",
      "Epoch: 126............. Loss: 0.2357\n",
      "pk: 72.82 %\n",
      "Learning rate: 0.0013463793\n",
      "Epoch: 127............. Loss: 0.2337\n",
      "pk: 72.27 %\n",
      "Learning rate: 0.0014002344\n",
      "Epoch: 128............. Loss: 0.4653\n",
      "pk: 72.69 %\n",
      "Learning rate: 0.0014002344\n",
      "Epoch: 129............. Loss: 0.1710\n",
      "pk: 72.41 %\n",
      "Learning rate: 0.0014562438\n",
      "Epoch: 130............. Loss: 0.2647\n",
      "pk: 72.19 %\n",
      "Learning rate: 0.0015144936\n",
      "Epoch: 131............. Loss: 0.1618\n",
      "pk: 72.86 %\n",
      "Learning rate: 0.0015144936\n",
      "Epoch: 132............. Loss: 0.1489\n",
      "pk: 73.29 %\n",
      "Learning rate: 0.0015750733\n",
      "Epoch: 133............. Loss: 0.0628\n",
      "pk: 73.03 %\n",
      "Learning rate: 0.0016380762\n",
      "Epoch: 134............. Loss: 0.2133\n",
      "pk: 72.61 %\n",
      "Learning rate: 0.0016380762\n",
      "Epoch: 135............. Loss: 0.2821\n",
      "pk: 72.66 %\n",
      "Learning rate: 0.0017035993\n",
      "Epoch: 136............. Loss: 0.2550\n"
     ]
    }
   ],
   "source": [
    "# input image size in px (square image)\n",
    "input_size = 32\n",
    "dataset_name = \"cifar\"\n",
    "epoch = 200\n",
    "\n",
    "methods = ['kaiming_uniform']\n",
    "for method in methods:\n",
    "    for apt in range(3):\n",
    "        model = TrainModel(dataset=CifarDataset(), method=method, input_size=input_size,\n",
    "                                c_kernels=[3, 3, 3, 3, 3], in_channels=[3, 16, 32, 64, 86, 128], out_channels=[16, 32, 64, 86, 128, 128], apt=apt, dataset_name=dataset_name, epoch=epoch, activation_relu=True)\n",
    "        sse, sse_t, acc, e = model.training()\n",
    "        np.savetxt(\"./output_data/data_plots/\" + dataset_name + method + \"_\" + str(apt) + \"shallow\" + \".csv\", sse, delimiter=\";\")\n",
    "        np.savetxt(\"./output_data/data_plots/\" + dataset_name + method + \"_\" + str(apt) + \"shallow\" + \"_t.csv\", sse_t, delimiter=\";\")\n",
    "        with open('./output_data/acc/' + dataset_name + method + \"_\" + str(apt) + \"shallow\" + '.txt', 'w') as f:\n",
    "            f.write('pk: ' + str(acc) + '\\n')\n",
    "        torch.save(model.cnn_model, \"./output_data/models/\" + dataset_name + method + \"_\" + \"shallow\" + str(apt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
      "class num: 6\n",
      "ilość klas: 6\n",
      "wielkość po warstawach conv: 4\n",
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "          Conv2d-1     [1, 16, 150, 150]           2,368           2,368\n",
      "            Tanh-2     [1, 16, 150, 150]               0               0\n",
      "          Conv2d-3     [1, 32, 150, 150]          25,120          25,120\n",
      "            Tanh-4     [1, 32, 150, 150]               0               0\n",
      "          Conv2d-5     [1, 64, 150, 150]         100,416         100,416\n",
      "            Tanh-6     [1, 64, 150, 150]               0               0\n",
      "       MaxPool2d-7       [1, 64, 75, 75]               0               0\n",
      "          Conv2d-8       [1, 86, 75, 75]         269,782         269,782\n",
      "            Tanh-9       [1, 86, 75, 75]               0               0\n",
      "         Conv2d-10      [1, 128, 75, 75]         275,328         275,328\n",
      "           Tanh-11      [1, 128, 75, 75]               0               0\n",
      "      MaxPool2d-12      [1, 128, 37, 37]               0               0\n",
      "         Conv2d-13      [1, 128, 37, 37]         409,728         409,728\n",
      "           Tanh-14      [1, 128, 37, 37]               0               0\n",
      "         Conv2d-15      [1, 128, 37, 37]         409,728         409,728\n",
      "           Tanh-16      [1, 128, 37, 37]               0               0\n",
      "      MaxPool2d-17      [1, 128, 18, 18]               0               0\n",
      "         Conv2d-18      [1, 128, 18, 18]         409,728         409,728\n",
      "           Tanh-19      [1, 128, 18, 18]               0               0\n",
      "         Conv2d-20      [1, 128, 18, 18]         409,728         409,728\n",
      "           Tanh-21      [1, 128, 18, 18]               0               0\n",
      "      MaxPool2d-22        [1, 128, 9, 9]               0               0\n",
      "         Conv2d-23        [1, 128, 9, 9]         409,728         409,728\n",
      "           Tanh-24        [1, 128, 9, 9]               0               0\n",
      "         Conv2d-25        [1, 128, 9, 9]         409,728         409,728\n",
      "           Tanh-26        [1, 128, 9, 9]               0               0\n",
      "      MaxPool2d-27        [1, 128, 4, 4]               0               0\n",
      "         Conv2d-28        [1, 128, 4, 4]         409,728         409,728\n",
      "           Tanh-29        [1, 128, 4, 4]               0               0\n",
      "         Linear-30              [1, 128]         262,272         262,272\n",
      "           Tanh-31              [1, 128]               0               0\n",
      "         Linear-32               [1, 64]           8,256           8,256\n",
      "           Tanh-33               [1, 64]               0               0\n",
      "         Linear-34               [1, 16]           1,040           1,040\n",
      "           Tanh-35               [1, 16]               0               0\n",
      "         Linear-36                [1, 6]             102             102\n",
      "=========================================================================\n",
      "Total params: 3,812,780\n",
      "Trainable params: 3,812,780\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n",
      "GPU is available\n",
      "pk: 48.37 %\n",
      "Learning rate: 0.0010000000\n",
      "Epoch: 0............. Loss: 1.3368\n"
     ]
    }
   ],
   "source": [
    "input_size = 150\n",
    "epoch = 50\n",
    "dataset_name = \"intel\"\n",
    "methods = ['kaiming_uniform']\n",
    "\n",
    "for method in methods:\n",
    "    for apt in range(3, 6):\n",
    "        model = TrainModel(dataset=IntelDataset(), method=method, input_size=input_size,\n",
    "                            c_kernels=[7, 7, 7, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5], in_channels=[3, 16, 32, 64, 86, 128, 128, 128, 128, 128, 128, 128], out_channels=[16, 32, 64, 86, 128, 128, 128, 128, 128, 128, 128, 128], apt=apt, dataset_name=dataset_name, epoch=epoch, padding_flag=True, maxpool_freq=2, activation_relu=True, fc_size=4)\n",
    "        sse, sse_t, acc, e = model.training()\n",
    "        np.savetxt(\"./output_data/data_plots/\" + method + \"_\" + str(apt) + \".csv\", sse, delimiter=\";\")\n",
    "        np.savetxt(\"./output_data/data_plots/\" + method + \"_\" + str(apt) + \"_t.csv\", sse_t, delimiter=\";\")\n",
    "        with open('./output_data/acc/' + method + \"_\" + str(apt) + '.txt', 'w') as f:\n",
    "            f.write('pk: ' + str(acc) + '\\n')\n",
    "        torch.save(model.cnn_model, \"./output_data/models/\" + method + \"_\" + str(apt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 150\n",
    "epoch = 200\n",
    "dataset_name = \"intel\"\n",
    "methods = ['kaiming_uniform']\n",
    "\n",
    "for method in methods:\n",
    "    for apt in range(3, 6):\n",
    "        model = TrainModel(dataset=IntelDataset(), method=method, input_size=input_size,\n",
    "                            c_kernels=[5, 5, 5, 5, 5, 5, 5], in_channels=[3, 16, 32, 64, 86, 128, 128, 128], out_channels=[16, 32, 64, 86, 128, 128, 128, 128], apt=apt, dataset_name=dataset_name, epoch=epoch, padding_flag=True, maxpool_freq=2, activation_relu=True, fc_size=4)\n",
    "        sse, sse_t, acc, e = model.training()\n",
    "        np.savetxt(\"./output_data/data_plots/\" + method + \"_\" + str(apt) + \"shallow\" + \".csv\", sse, delimiter=\";\")\n",
    "        np.savetxt(\"./output_data/data_plots/\" + method + \"_\" + str(apt) + \"shallow\" + \"_t.csv\", sse_t, delimiter=\";\")\n",
    "        with open('./output_data/acc/' + method + \"_\" + str(apt) + \"shallow\" + '.txt', 'w') as f:\n",
    "            f.write('pk: ' + str(acc) + '\\n')\n",
    "        torch.save(model.cnn_model, \"./output_data/models/\" + method + \"_\" + \"shallow\" + str(apt))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483af022b5ec7d24700bcdf0e25d1e2b25f3a954a5b2083e220785423700196e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
